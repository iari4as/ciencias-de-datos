{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be68690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline_red.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comentario. Importar librerías\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "class LimpiarComillas(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    • Limpia comillas simples y espacios en headers y celdas.\n",
    "    • Quita espacios internos SÓLO en valores numéricos.\n",
    "    • Convierte strings numéricos a float (NaN si falla) cuando cast_numeric=True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cast_numeric: bool = True):\n",
    "        self.cast_numeric = cast_numeric\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_cell(val, cast_numeric):\n",
    "        if not isinstance(val, str):\n",
    "            return val\n",
    "        v = val.strip(\" '\")\n",
    "        if re.fullmatch(r\"[0-9\\.,\\s]+\", v):\n",
    "            v_num = v.replace(\" \", \"\")\n",
    "            if cast_numeric:\n",
    "                if \",\" in v_num and \".\" not in v_num:\n",
    "                    v_num = v_num.replace(\",\", \".\")\n",
    "                return pd.to_numeric(v_num, errors=\"coerce\")\n",
    "            return v_num\n",
    "        return v\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Limpiar nombres de columnas\n",
    "        cols_raw = X.columns.astype(str).str.strip(\" '\")\n",
    "        seen, cols_clean = {}, []\n",
    "        for c in cols_raw:\n",
    "            cnt = seen.get(c, 0)\n",
    "            cols_clean.append(f\"{c}_{cnt}\" if cnt else c)\n",
    "            seen[c] = cnt + 1\n",
    "        X.columns = cols_clean\n",
    "\n",
    "        # Limpiar celdas en columnas object o string\n",
    "        obj_cols = X.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "        X[obj_cols] = X[obj_cols].applymap(\n",
    "            lambda v: self._clean_cell(v, self.cast_numeric)\n",
    "        )\n",
    "        return X\n",
    "\n",
    "class ConvertirObjectAString(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convierte columnas de tipo 'object' o 'string' a tipo 'string' nativo de pandas.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(include=[\"object\", \"string\"]).columns:\n",
    "            X[col] = X[col].astype(\"string\")\n",
    "        return X\n",
    "\n",
    "# Pipeline básico de limpieza\n",
    "pipeline_red = Pipeline([\n",
    "    (\"convertir_a_str\", ConvertirObjectAString()),\n",
    "    (\"strip\", LimpiarComillas())\n",
    "])\n",
    "\n",
    "def crear_pipeline_completo(X):\n",
    "    \"\"\"\n",
    "    Construye un Pipeline completo con:\n",
    "    - Limpieza de comillas y conversión de objetos a string (pipeline_red)\n",
    "    - Escalado de columnas numéricas\n",
    "    - Codificación one-hot de columnas categóricas (salida sparse para ahorro de memoria)\n",
    "    \"\"\"\n",
    "    # Identificar columnas categóricas y numéricas\n",
    "    columnas_categoricas = X.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "    columnas_numericas = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "    # ColumnTransformer con salida sparse\n",
    "    preprocesador = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), columnas_numericas),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, dtype=np.float32), columnas_categoricas)\n",
    "    ], sparse_threshold=0.0)\n",
    "\n",
    "    pipeline_completo = Pipeline([\n",
    "        (\"limpieza\", pipeline_red),\n",
    "        (\"prepro\", preprocesador)\n",
    "    ])\n",
    "    return pipeline_completo\n",
    "\n",
    "\n",
    "# Comentario. Guardar a disco\n",
    "joblib.dump(pipeline_red, \"pipeline_red.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Snipe\\OneDrive\\Escritorio\\_\\USM 2025-1 CIENCIA DE DATOS\\taller3\\transformadores_red.py:56: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X[obj_cols] = X[obj_cols].applymap(\n",
      "c:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101656 entries, 0 to 101655\n",
      "Data columns (total 43 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       101656 non-null  string\n",
      " 1   1       101656 non-null  string\n",
      " 2   2       101656 non-null  string\n",
      " 3   3       101656 non-null  string\n",
      " 4   4       101656 non-null  string\n",
      " 5   5       101656 non-null  string\n",
      " 6   6       101656 non-null  string\n",
      " 7   7       101656 non-null  string\n",
      " 8   8       101656 non-null  string\n",
      " 9   9       101656 non-null  string\n",
      " 10  10      101656 non-null  string\n",
      " 11  11      101656 non-null  string\n",
      " 12  12      101656 non-null  string\n",
      " 13  13      101656 non-null  string\n",
      " 14  14      101656 non-null  string\n",
      " 15  15      101656 non-null  string\n",
      " 16  16      101656 non-null  string\n",
      " 17  17      101656 non-null  string\n",
      " 18  18      101656 non-null  string\n",
      " 19  19      101656 non-null  string\n",
      " 20  20      101656 non-null  string\n",
      " 21  21      101656 non-null  string\n",
      " 22  22      101656 non-null  string\n",
      " 23  23      101656 non-null  string\n",
      " 24  24      101656 non-null  string\n",
      " 25  25      101656 non-null  string\n",
      " 26  26      101656 non-null  string\n",
      " 27  27      101656 non-null  string\n",
      " 28  28      101656 non-null  string\n",
      " 29  29      101656 non-null  string\n",
      " 30  30      101656 non-null  string\n",
      " 31  31      101656 non-null  string\n",
      " 32  32      101656 non-null  string\n",
      " 33  33      101656 non-null  string\n",
      " 34  34      101656 non-null  string\n",
      " 35  35      101656 non-null  string\n",
      " 36  36      101656 non-null  string\n",
      " 37  37      101656 non-null  string\n",
      " 38  38      101656 non-null  string\n",
      " 39  39      101656 non-null  string\n",
      " 40  40      101656 non-null  string\n",
      " 41  41      101656 non-null  string\n",
      " 42  42      101656 non-null  string\n",
      "dtypes: string(43)\n",
      "memory usage: 33.3 MB\n"
     ]
    }
   ],
   "source": [
    "import os, joblib, pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras import layers, models\n",
    "import joblib, pandas as pd, numpy as np\n",
    "from transformadores_red import LimpiarComillas      \n",
    "\n",
    "pipeline_red = joblib.load('pipeLine_red.pkl')\n",
    "df = pd.read_csv(\n",
    "    \"Muetra_Taller3.csv\",\n",
    "    header=None,\n",
    "    quotechar=\"'\",\n",
    "    skipinitialspace=True,\n",
    "    dtype=str\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_procesado = pipeline_red.transform(df)         \n",
    "df_procesado.head()\n",
    "\n",
    "datos = df_procesado.to_numpy()\n",
    "df_procesado.head()\n",
    "\n",
    "df_procesado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeo de clases: {\" 'Ataque'\": np.int64(0), \" 'Normal'\": np.int64(1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Snipe\\OneDrive\\Escritorio\\_\\USM 2025-1 CIENCIA DE DATOS\\taller3\\transformadores_red.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  )\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.62 GiB for an array with shape (101655, 17481) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Paso 4: Crear pipeline y transformar X\u001b[39;00m\n\u001b[32m     35\u001b[39m pipeline_completo = crear_pipeline_completo(X)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m X_procesado       = \u001b[43mpipeline_completo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Paso 5: Convertir a arrays NumPy\u001b[39;00m\n\u001b[32m     39\u001b[39m X_array = np.asarray(X_procesado, dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat64\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:730\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    724\u001b[39m last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    725\u001b[39m     step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    726\u001b[39m     step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    727\u001b[39m     all_params=params,\n\u001b[32m    728\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step.fit(Xt, y, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m]).transform(\n\u001b[32m    735\u001b[39m         Xt, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    736\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1031\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_output(Xs)\n\u001b[32m   1029\u001b[39m \u001b[38;5;28mself\u001b[39m._record_output_indices(Xs)\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1147\u001b[39m, in \u001b[36mColumnTransformer._hstack\u001b[39m\u001b[34m(self, Xs, n_samples)\u001b[39m\n\u001b[32m   1145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sparse.hstack(converted_Xs).tocsr()\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     Xs = [\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(f) \u001b[38;5;28;01melse\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m Xs]\n\u001b[32m   1148\u001b[39m     adapter = _get_container_adapter(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(adapter.is_supported_container(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m Xs):\n\u001b[32m   1150\u001b[39m         \u001b[38;5;66;03m# rename before stacking as it avoids to error on temporary duplicated\u001b[39;00m\n\u001b[32m   1151\u001b[39m         \u001b[38;5;66;03m# columns\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1170\u001b[39m, in \u001b[36m_cs_matrix.toarray\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1169\u001b[39m     order = \u001b[38;5;28mself\u001b[39m._swap(\u001b[33m'\u001b[39m\u001b[33mcf\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out.flags.c_contiguous \u001b[38;5;129;01mor\u001b[39;00m out.flags.f_contiguous):\n\u001b[32m   1172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mOutput array must be C or F contiguous\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\_base.py:1367\u001b[39m, in \u001b[36m_spbase._process_toarray_args\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m   1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 6.62 GiB for an array with shape (101655, 17481) and data type float32"
     ]
    }
   ],
   "source": [
    "# Paso 1: Importar librerías y cargar datos\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import importlib\n",
    "import transformadores_red\n",
    "importlib.reload(transformadores_red)\n",
    "from transformadores_red import crear_pipeline_completo\n",
    "import numpy as np\n",
    "\n",
    "# —— Carga y limpieza básica —— \n",
    "df_procesado = pd.read_csv(\"Muetra_Taller3.csv\", encoding=\"utf-8\")\n",
    "# Si tus columnas tienen comillas o espacios extra:\n",
    "df_procesado.columns = (\n",
    "    df_procesado.columns\n",
    "        .str.strip()\n",
    "        .str.replace(\"'\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# Separar columnas\n",
    "ids         = df_procesado.iloc[:, 0]     # primera columna: ID\n",
    "X           = df_procesado.iloc[:, 1:-1]  # de la 2ª a la penúltima: features\n",
    "y           = df_procesado.iloc[:, -1]    # última columna: etiqueta\n",
    "\n",
    "# Paso 3: Codificar variable objetivo\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(\"Mapeo de clases:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "#Crear pipeline y transformar X\n",
    "pipeline_completo = crear_pipeline_completo(X)\n",
    "X_procesado       = pipeline_completo.fit_transform(X)\n",
    "\n",
    "# Paso 5: Convertir a arrays NumPy\n",
    "X_array = np.asarray(X_procesado, dtype=\"float64\")\n",
    "y_array = np.asarray(y_encoded,  dtype=\"float64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7744a3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9869 - loss: 0.5659 - val_accuracy: 0.9896 - val_loss: 0.2976\n",
      "Epoch 2/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9902 - loss: 0.2490 - val_accuracy: 0.9896 - val_loss: 0.1470\n",
      "Epoch 3/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9898 - loss: 0.1277 - val_accuracy: 0.9896 - val_loss: 0.0877\n",
      "Epoch 4/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.0777 - val_accuracy: 0.9896 - val_loss: 0.0659\n",
      "Epoch 5/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0635 - val_accuracy: 0.9896 - val_loss: 0.0593\n",
      "Epoch 6/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.0561 - val_accuracy: 0.9896 - val_loss: 0.0580\n",
      "Epoch 7/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9902 - loss: 0.0554 - val_accuracy: 0.9896 - val_loss: 0.0578\n",
      "Epoch 8/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9898 - loss: 0.0569 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 9/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.0547 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 10/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0573 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 11/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0562 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 12/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.0547 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 13/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9899 - loss: 0.0566 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 14/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0574 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 15/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9899 - loss: 0.0563 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 16/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9908 - loss: 0.0521 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 17/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9902 - loss: 0.0553 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 18/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9892 - loss: 0.0595 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 19/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9899 - loss: 0.0564 - val_accuracy: 0.9896 - val_loss: 0.0577\n",
      "Epoch 20/20\n",
      "\u001b[1m2542/2542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9902 - loss: 0.0552 - val_accuracy: 0.9896 - val_loss: 0.0577\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Definir arquitectura mejorada\n",
    "model = Sequential()\n",
    "model.add(Dense(124, activation='relu', input_dim=X_array.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar con optimizador ajustado\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Callback para detener si no mejora\n",
    "\n",
    "# Entrenar el modelo con validación y early stopping\n",
    "model.fit(X_array, y_array,\n",
    "          epochs=20,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)\n",
    "\n",
    "# Guardar el modelo\n",
    "model.save(\"modelo_ANN.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Snipe\\OneDrive\\Escritorio\\_\\USM 2025-1 CIENCIA DE DATOS\\taller3\\transformadores_red.py:56: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X[obj_cols] = X[obj_cols].applymap(\n",
      "c:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Snipe\\OneDrive\\Escritorio\\_\\USM 2025-1 CIENCIA DE DATOS\\taller3\\transformadores_red.py:56: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X[obj_cols] = X[obj_cols].applymap(\n",
      "c:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Snipe\\OneDrive\\Escritorio\\_\\USM 2025-1 CIENCIA DE DATOS\\taller3\\transformadores_red.py:56: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X[obj_cols] = X[obj_cols].applymap(\n",
      "c:\\Users\\Snipe\\OneDrive\\Escritorio\\_\\USM 2025-1 CIENCIA DE DATOS\\taller3\\transformadores_red.py:56: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X[obj_cols] = X[obj_cols].applymap(\n",
      "c:\\Users\\Snipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluación completada y archivo 'inspeccion.txt' generado.\n"
     ]
    }
   ],
   "source": [
    "#importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "from transformadores_red import crear_pipeline_completo, pipeline_red\n",
    "\n",
    "#cargar los datos de evaluación\n",
    "df_nuevo = pd.read_csv(\"Muetra_Taller3_Evaluacion.csv\",\n",
    "                       header=None,\n",
    "                       quotechar=\"'\",\n",
    "                       skipinitialspace=True,\n",
    "                       dtype=str)\n",
    "\n",
    "#aplicar pipeline de limpieza (comillas, espacios, tipos)\n",
    "df_limpio = pipeline_red.transform(df_nuevo)\n",
    "\n",
    "# seleccionar las columnas de entrada (sin ID y sin target)\n",
    "X_eval = df_limpio.iloc[:, 1:]  # columnas de la 1 a la última\n",
    "\n",
    "#econstruir el pipeline completo con las columnas del set limpio\n",
    "pipeline_completo = crear_pipeline_completo(X_eval)\n",
    "\n",
    "#Paentrenar el pipeline SOLO con los datos originales \n",
    "df_entrenamiento = pd.read_csv(\"Muetra_Taller3.csv\",\n",
    "                                header=None,\n",
    "                                quotechar=\"'\",\n",
    "                                skipinitialspace=True,\n",
    "                                dtype=str)\n",
    "\n",
    "df_entrenamiento_limpio = pipeline_red.transform(df_entrenamiento)\n",
    "X_train = df_entrenamiento_limpio.iloc[:, 1:-1]  # sin ID, sin target\n",
    "\n",
    "pipeline_completo.fit(X_train)  #aquí se entrena el preprocesador!\n",
    "\n",
    "# transformar los datos de evaluación\n",
    "X_eval_array = pipeline_completo.transform(X_eval)\n",
    "\n",
    "modelo = load_model(\"modelo_ANN.keras\")\n",
    "\n",
    "predicciones = modelo.predict(X_eval_array, verbose=0).flatten()\n",
    "\n",
    "#guardar 150 IDs más altos\n",
    "ids = df_nuevo.iloc[:, 0]\n",
    "top_150_indices = np.argsort(predicciones)[-150:][::-1]\n",
    "top_150_ids = ids.iloc[top_150_indices]\n",
    "\n",
    "with open(\"inspeccion.txt\", \"w\") as f:\n",
    "    f.write(\",\".join(map(str, top_150_ids.tolist())))\n",
    "\n",
    "print(\"✅ Evaluación completada y archivo 'inspeccion.txt' generado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c922958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
