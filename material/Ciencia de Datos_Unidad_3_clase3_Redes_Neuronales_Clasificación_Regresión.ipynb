{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26092db2",
   "metadata": {},
   "source": [
    "<head><b><center>CIENCIA DE DATOS</center>\n",
    "\n",
    "<center>APRENDIZAJE SUPERVISADO</center>\n",
    "<center>Profundizando en Redes Neuronales Artificiales: Clasificación y Regresión</center></b>\n",
    "    \n",
    "\n",
    "<center>Profesor: Gabriel Jara </center></head><br>\n",
    "El presente Jupyter Notebook busca:\n",
    "<ul>\n",
    "    <li>Avanzar nuestro dominio sobre el uso de Redes Neuronales Artificiales.</li>\n",
    "    <li>Explicar el mecanismo de aprendizaje de las redes neuronales.</li>\n",
    "    <li>Explicar el rol de la función de pérdida y optimización.</li>\n",
    "    <li>Aplicar modelos de ANN a problemas de regresión.</li>\n",
    "</ul>\n",
    "\n",
    "Nota: Las imágenes que han sido robadas de internet son enlaces a su correspondiente fuente. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d672aa9",
   "metadata": {},
   "source": [
    "<b>EN EL CAPÍTULO ANTERIOR </b>\n",
    "\n",
    "Presentamos las Redes Neuronales Artificiales como modelo de aprendizaje. Identificamos el <b><i>Perceptron</i></b> como unidad básica de este modelo, cuya función emula el rol de una neurona: percibe un estímulo según el cual se activa (o no) y con ello transmite una señal. \n",
    "<br> <img src=https://images.deepai.org/glossary-terms/perceptron-6168423.jpg\n",
    "          width=\"400\"/>\n",
    "\n",
    "La naturaleza limitada de un perceptrón no permitiría que resolviera problemas muy complejos. En el contexto de Clasificación, sólo podría clasificar observaciones que fueran linealmente separables. Pero organizado en nodos de una red distribuida en capas densamente conectadas, alcanzan capacidades mucho mayores. Esta es la definición básica de una red neuronal, denominada <b><i>Multi-Layer Perceptron</i></b>. \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:563/1*4_BDTvgB6WoYVXyxO8lDGA.png\n",
    "          width=\"350\"/>\n",
    "          \n",
    "Usamos redes neuronales de varias capas para clasificar a los pasajeros del Titanic, llegando a un accuracy de testeo por sobre 80%. Para llegar a este resultado sólo tuvimos que importar el constructor de redes densas de Keras, apilar las capas, configurar la cantidad de epochs de entrenamiento. Por ahora hemos explorado muy pocas configuraciones de red neuronal. \n",
    "\n",
    "La mayor dificultad que hemos encontrado, hasta acá, es que nuestros modelos son tan poderosos en capacidad de representación que fácilmente sobre-ajustan (overfitting), lo cual perjudica su capacidad de generalizar el aprendizaje. \n",
    "\n",
    "<img src=https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/overfitting_2.png width=\"400\"/>\n",
    "\n",
    "¿Qué más podríamos hacer con redes neuronales?        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fb832",
   "metadata": {},
   "source": [
    "<b>CLASIFICACIÓN DE IMAGENES</b>\n",
    "\n",
    "Ya nos hemos aburrido del Titanic, al final siempre se hunde. Cuando se habla de Redes Neuronales Artificiales suele ser asociado a tareas mucho más interesantes, las que atribuimos al concepto de Inteligencia Artificial. Tratemos de adentrarnos un poco en ese mundo, para lo cual podemos tomar como referencia el problema de clasificar imágenes. \n",
    "\n",
    "Vamos a usar un dataset disponible en Keras, de forma que no necesitaremos datos propios. Se trata de <b><i>Mnist</i></b>, una colección de imágenes de dígitos (números) escritos a mano por distintas personas. \n",
    "<br> <img src=https://www.ttested.com/gallery/thumbnails/ditch-mnist.jpg\n",
    "          width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be668269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 28, 28)\n",
      "-  -  -  -  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  -  -  -  -\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0   13   21    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0   12   77   77   77  180  222  185  185  131   77   49    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0   88  254  254  254  254  254  254  254  254  254  243  214   89    2    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0  212  254  254  254  254  254  254  254  254  254  254  254  254  114    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0  174  254  254  254  254  254  254  254  254  254  254  254  254  230   43    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0   48  244  195   70   27   27   27   27   80  135  221  254  254  254  217    4    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0  172  254  254  254  148    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0   47  254  254  254  239   33    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    2  168  254  254  254   50    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0   19   55   55   55    4   48  244  254  254  173    1    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    4   93  194  254  254  254  204  175  224  254  254  254    5    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    7  171  254  254  254  254  254  254  254  254  254  254  254    5    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0  108  254  254  254  254  254  254  254  254  254  254  254  254    5    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0   78  242  254  254  254  254  254  254  254  254  254  254  254  254   47    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0  152  254  254  254  254  102   38  124  248  254  254  254  254  254  175   20    0  0  0  0  0\n",
      "0  0  0  0    0    0    0   79  254  254  254  254  254  254  254  254  254  254  254  255  254  254  232  128  0  0  0  0\n",
      "0  0  0  0    0    0    0    8  201  254  254  254  254  254  254  254  254  254  254  254  254  254  254  210  0  0  0  0\n",
      "0  0  0  0    0    0    0    0  130  254  254  254  254  254  254  224  150   67  175  254  254  254  254  132  0  0  0  0\n",
      "0  0  0  0    0    0    0    0   25  176  254  254  250  160   76    5    0    0   20  184  243  254  235   27  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    2   38   38   35    0    0    0    0    0    0    0   31   38   27    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "-  -  -  -  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  -  -  -  -\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n",
      "\n",
      "-  -  -  -  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  -  -  -  -\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0   63  178  171  241  241  128   23    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0  147  253  254  253  253  253  233  109    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0  101  173  254  253  253  253  253  245   68    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0   40  138  254  253  253  253  253  253  196   25    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0   39   66   67   66   66  224  253  253  245   74    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0  214  253  253  213    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0  214  253  253  213    0    0   15  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0  214  253  253  236  147  147  203  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0  214  253  253  253  253  253  253  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0   64  122  177  254  254  254  254  255  255  255  255  0  0  0  0\n",
      "0  0  0  0    0    0    0    0   29  108  128  241  241  247  254  253  253  253  253  253  253  185  146  146  0  0  0  0\n",
      "0  0  0  0    0   90   94  207  234  253  253  253  253  253  254  253  253  253  253  233   68   10    0    0  0  0  0  0\n",
      "0  0  0  0  103  251  253  253  253  253  253  253  253  253  254  253  253  253  196   31    0    0    0    0  0  0  0  0\n",
      "0  0  0  0  227  253  253  196  186  186  165   53   53   88  254  253  253  232   67    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0  254  253  253   68    0    0    9   54  131  222  254  253  233   66    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0  162  253  253  228  174  174  187  253  253  253  255  236   64    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0  109  252  253  253  253  253  253  253  253  253  228   60    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0  101  226  240  240  240  240  240  162  107    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "0  0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0\n",
      "-  -  -  -  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  -  -  -  -\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from keras.datasets import mnist\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2))\n",
    "\n",
    "print(np.array(X).shape)\n",
    "\n",
    "print(tabulate.tabulate(X[0]))\n",
    "fig = plt.figure\n",
    "plt.imshow(X[0], cmap='gray_r')\n",
    "plt.show()\n",
    "print(y[0])\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(tabulate.tabulate(X[1]))\n",
    "fig = plt.figure\n",
    "plt.imshow(X[1], cmap='gray_r')\n",
    "plt.show()\n",
    "print(y[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c97d5",
   "metadata": {},
   "source": [
    "Podemos apreciar que Mnist almacena imágenes en blanco y negro (un único canal), de 28x28 pixeles. Hubo una época oscura, antes de que la IA iluminara nuestras vidas, en que reconocer texto escrito a mano era considerado un problema difícil de resolver para una computadora. Hoy se trata de un problema trivial, pero datasets como Mnist todavía se usan con fines educativos y también de investigación. Una de las formas de reportar que un modelo es más exitoso que otro es comparando sus resultados sobre sets de datos ya conocidos y disponibles para toda la comunidad. \n",
    "\n",
    "Usemos una ANN para clasificar Mnist. Como sólo es una demostración no vamos a usar todos los datos disponibles. Creemos un set de entrenamiento de 1000 imágenes, y dado que tenemos de sobra, saquemos otro set de 1000 imágenes para testeo. Así que partiremos por tomar 2000 imágenes para preprocesarlas y luego dividirlas en partes iguales para entrenamiento y testeo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64a37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "random.seed(123) # Vamos a controlar la aleatoriedad en adelante. \n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b62d745",
   "metadata": {},
   "source": [
    "Tenemos una colección de imágenes de 28x28 pixeles para entrenar... ¿como le metemos fotos a una red neuronal? \n",
    "\n",
    "Cada pixel de las imágenes es un número, así que al menos en eso no estamos mal. Según vimos nuestras ANN pueden recibir vectores, pero una imagen es una matriz, tiene dos dimensiones en lugar de sólo una. La solución es simple y directa, reacomodaremos cada imagen para que sea un vector de 784 números, en lugar de una matriz de 28x28. Puede imaginarlo como que recorta cada foto de arriba a abajo en 28 tiras, las pega a lo largo para formar una tira larga y le muestra eso a la Red Neuronal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85347438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño antes de rearreglar:  28 x 28\n",
      "Tamaño después de rearreglar:  784\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño antes de rearreglar: ',len(X[0]),'x',len(X[0][0]))\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "print('Tamaño después de rearreglar: ',len(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956313ae",
   "metadata": {},
   "source": [
    "Dijimos que aplicaremos normalización, así que vamos a usar Min-Max para estos efectos, aprovechando que ya está disponible en sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9f417e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 159. 239.  13.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  95. 253. 188.   3.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  20. 226. 254.  75.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. 104. 254. 243.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. 179. 254. 178.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  56. 254. 254.  70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  96. 254. 190.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  18.\n",
      " 217. 254.  84.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  60.\n",
      " 254. 214.  18.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1. 153.\n",
      " 254. 140.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4. 254.\n",
      " 254.  36.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 116. 255.\n",
      " 197.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 226. 245.\n",
      "  62.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 253. 189.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3. 206. 253.  50.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  42. 254. 199.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 159. 254.  46.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  19. 233. 189.   3.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0. 137. 253.  53.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  60. 247. 136.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "\n",
      "Después:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.62352941 0.9372549  0.05098039 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.37254902 0.99215686\n",
      " 0.7372549  0.01176471 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.07843137 0.88627451 0.99607843 0.29411765 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.40784314\n",
      " 0.99607843 0.95294118 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.70196078 0.99607843 0.69803922\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21960784 0.99607843 0.99607843 0.2745098  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.37647059 0.99607843\n",
      " 0.74509804 0.01568627 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.07058824 0.85098039 0.99607843 0.32941176 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.23529412\n",
      " 0.99607843 0.83921569 0.07058824 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00392157 0.6        0.99607843 0.54901961\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01568627 0.99607843 0.99607843 0.14117647 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.45490196 1.\n",
      " 0.77254902 0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.88627451 0.96078431 0.24313725 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.4745098\n",
      " 0.99215686 0.74117647 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.80784314 0.99215686 0.19607843\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.16470588 0.99607843 0.78039216 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.62352941 0.99607843\n",
      " 0.18039216 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0745098  0.91372549 0.74117647 0.01176471 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.5372549\n",
      " 0.99215686 0.20784314 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.23529412 0.96862745 0.53333333 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "print('Antes: ',X[0])\n",
    "print()\n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "print('Después: ',X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332f8d8",
   "metadata": {},
   "source": [
    "También necesitamos cambiar la forma de la variable dependiente $y$, que en este momento es un entero en el rango 0 a 9. La ANN va a tener 10 nodos de salida, por lo que sus resultados estarán en la forma de un vector de tamaño 10, de los que esperamos haya sólo un valor 1 y el resto sean 0, la posición donde esté el 1 marcará la clase predicha. Por lo tanto, necesitamos entrenar y testear con datos cuya etiqueta tenga esa forma. Esto es algo que ya hemos visto antes, lo podemos lograr transformando el dato a una representación categórica.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e17797e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes:  6.0\n",
      "Después:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "print('Antes: ',y[1])\n",
    "y = to_categorical(y)\n",
    "print('Después: ',y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d24d01",
   "metadata": {},
   "source": [
    "Ya estamos en condiciones de dividir en dos la muestra, para entrenamiento y testeo. Hemos esperado a completar el preprocesamiento para facilitar la etapa de testeo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c3d4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c454479",
   "metadata": {},
   "source": [
    "Probemos como le va a la ANN más simple que hicimos en la clase anterior, una capa con 8 nodos. ¿Será suficiente para aprender a leer los números?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71324c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 1ms/step - loss: 0.3578 - accuracy: 0.2830\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1970 - accuracy: 0.6470\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1415 - accuracy: 0.7840\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.8250\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1025 - accuracy: 0.8550\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0912 - accuracy: 0.8730\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0824 - accuracy: 0.8920\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1s 924us/step - loss: 0.0756 - accuracy: 0.9040\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.0696 - accuracy: 0.9160\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0639 - accuracy: 0.9250\n",
      "32/32 [==============================] - 0s 929us/step - loss: 0.1141 - accuracy: 0.8210\n",
      "Loss: 0.11405720561742783\n",
      "Accuracy: 0.8209999799728394\n",
      "Se demoró 0.23 minutos.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Creamos un modelo de la red neuronal declarando una secuencia a la que agregaremos capas. \n",
    "model = Sequential()\n",
    "\n",
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende de la cantidad de Xs.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Agregamos la primera capa oculta, donde además declaramos la dimensión de la capa de entrada.\n",
    "# Debemos establecer cuantos nodos tiene esta capa oculta.\n",
    "# Debemos declarar la función de activación de estos nodos. \n",
    "model.add(Dense(units=8, activation='relu', input_dim=entrada_dim))\n",
    "\n",
    "# Agregamos una capa de salida con 10 nodos ÚNICO CAMBIO RESPECTO A LA RED QUE USAMOS CON TITANIC\n",
    "# Declaramos la función de activación del nodo de salida. \n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "\n",
    "# Compilamos el modelo indicando la función de perdida, optimizador y métricas.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Registremos cuanto se demora. \n",
    "inicio = time.time()\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento\n",
    "# Haremos 10 iteraciones de entrenamiento\n",
    "# en cada una de ellas los datos de entrenamiento se usarán de uno en uno.\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "\n",
    "# Evaluamos el modelo con datos de testeo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Se demoró {:.2f} minutos.'.format(duración/60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58848d",
   "metadata": {},
   "source": [
    "Podemos observar que se logra del orden de 70% ~ 80% de accuracy de testeo. Puede que no suene tan impresionante, pero tomemos en cuenta una diferencia importante respecto al Titanic, ¿cuál sería la exactitud de un modelo tonto? Ya fuera que asignáramos la etiqueta al azar o que usemos la moda para etiquetarlas a todas, en el mejor de los casos apenas superaríamos 10% de accuracy. Cualquier clasificador que logra significativamente más de 10% en este problema debe estar aprendiendo. \n",
    "\n",
    "Lo que sigue es definir un modelo de ANN más poderoso y entrenarlo en cantidad suficiente para que logre aprender a clasificar, pero antes de que el overfitting degrade su capacidad de generalización. Podríamos usar un grid search y cross validation, pero por tiempo de ejecución, para esta experiencia vamos a usar una cantidad fija de 60 epochs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4253e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 90   0   1   0   1   3   5   0   0   1]\n",
      " [  0 102   4   1   0   1   0   0   1   0]\n",
      " [  1   1  90   2   5   0   5   0   5   0]\n",
      " [  1   5   4  81   0   2   0   1   5   1]\n",
      " [  0   0   2   0  84   1   2   0   1   5]\n",
      " [  1   0   1   3   0  77   3   0   3   1]\n",
      " [  1   1   0   0   2   2 106   0   0   0]\n",
      " [  0   4   1   0   1   0   0  88   0   6]\n",
      " [  3   0   3  10   1   6   2   0  68   0]\n",
      " [  1   0   0   1   3   3   0   2   1  81]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Definimos el modelo de tipo ANN\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamos 60 épocas\n",
    "model.fit(X_train, y_train, epochs=60,\n",
    "          batch_size=32,\n",
    "          verbose = 0)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Las clases en los datos de testeo y entrenamiento, en \n",
    "# este momento, son vectores. Para poder evaluar nos conviene \n",
    "# más que vuelva a ser un campo numérico.\n",
    "y_test = [list(a) for a in y_test]\n",
    "y_hat = [list(a) for a in y_hat]\n",
    "y_test = [ a.index(max(a)) for a in y_test]\n",
    "y_hat = [ a.index(max(a)) for a in y_hat]\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f08a3e",
   "metadata": {},
   "source": [
    "<b>HIPERPARÁMETROS DE OPTIMIZACIÓN</b>\n",
    "\n",
    "Revisemos a continuación qué otros hiperparámetros tenemos disponibles para buscar una mejor configuración de la red neuronal. \n",
    "\n",
    "<b>Función de costo / Perdida / Error</b>\n",
    "\n",
    "Corresponde a la métrica con la que se evalúa el error de la predicción durante el entrenamiento. Esto significa que será dicha función la que se busca minimizar, y por tanto condiciona hacia donde buscará moverse la red neuronal. En problemas de clasificación las métricas a utilizar como perdida necesitan hacer referencia a la información que la predicción consigue capturar respecto a las observaciones reales, por eso lo más típico es usar medidas de entropía de información. \n",
    "\n",
    "<ul><li><b>Entropía Cruzada Binaria(Binary Crossentropy)</b> es la que ya estamos usando, Se utiliza en problemas de clasificación binaria donde hay dos clases. Nosotros lo estamos usando en un problema multiclase, pero atención que en cada nodo de salida nuestra red evalúa problemas de clasificación binarios, por lo que funciona.</li>\n",
    "    <li><b>Entropía Cruzada Categórica (Categorical Crossentropy)</b> Se utiliza en problemas de clasificación multiclase donde las clases se representan mediante codificación one-hot. Es apropiada cuando las clases son mutuamente excluyentes y el objetivo es asignar una única clase a cada muestra. Debieramos probarlo en el problema de Mnist.</li>\n",
    "    <li><b>Entropía Cruzada Categórica Dispersa (Sparse Categorical Crossentropy)</b> Similar a la entropía cruzada categórica, pero se utiliza cuando las clases se representan mediante etiquetas enteras en lugar de codificación one-hot. Es útil cuando hay un gran número de clases y la representación one-hot es ineficiente.</li></ul>\n",
    "\n",
    "No vamos a adentrarnos más en el cálculo de estas métricas cuyo origen está en teoría de la información y nos tomaría tiempo lograr una comprensión básica de ellas. La clase pasada ya exploramos el concepto de Entropía de Shanon. \n",
    "\n",
    "Probemos a continuación nuestra red neuronal usando Categorical Crossentropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b43f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 93   0   2   0   0   1   4   0   0   1]\n",
      " [  0 104   2   1   0   0   0   0   2   0]\n",
      " [  1   1  88   2   4   0   6   3   4   0]\n",
      " [  0   1   1  88   0   1   1   2   4   2]\n",
      " [  0   0   2   0  84   2   0   1   0   6]\n",
      " [  2   0   0   2   0  78   1   0   6   0]\n",
      " [  1   1   2   0   7   6  94   0   1   0]\n",
      " [  0   4   0   0   1   1   0  84   1   9]\n",
      " [  3   2   0   7   2   5   2   1  71   0]\n",
      " [  0   1   0   1   5   2   0   5   2  76]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Definimos el modelo de tipo ANN \n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "# ACÁ CAMBIAMOS LA FUNCIÓN DE PERDIDA\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Le pasamos la mejor epoch\n",
    "model.fit(X_train, y_train, epochs=60,\n",
    "          batch_size=32,\n",
    "          verbose = 0)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "\n",
    "# El set de datos de testeo y el entrenamiento, en este momento, son vectores.\n",
    "# Para poder evaluar nos conviene más que vuelva a ser un campo numérico\n",
    "y_test = [list(a) for a in y_test]\n",
    "y_hat = [list(a) for a in y_hat]\n",
    "y_test = [ a.index(max(a)) for a in y_test]\n",
    "y_hat = [ a.index(max(a)) for a in y_hat]\n",
    "\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117208f",
   "metadata": {},
   "source": [
    "<b>Función de Activación</b>\n",
    "\n",
    "En las ANN se ha usado distintas funciones de activación. Una de las más clásicas es <b>Sigmoid</b>, que es un caso de la que ya conocemos como función logística.\n",
    "\n",
    "$$Sig(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "<br> <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png\n",
    "          width=\"300\"/>\n",
    "\n",
    "Su gracia es que da resultados que tienden a 0 o 1, con un $x=0$ como umbral crítico. Esto es lo que la hace útil como función de activación de nodos de salida en un problema de clasificación.\n",
    "\n",
    "<b>Tanh (Tangente hiperbólica)</b> es similar a Sigmoidal pero con rango -1 y 1. El efecto es parecido al que produce la función sigmoidal, pero mapea explícitamente valores negativos. \n",
    "\n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:750/format:webp/1*f9erByySVjTjohfFdNkJYQ.jpeg\n",
    "          width=\"300\"/>\n",
    "\n",
    "Otra función de activación útil en los nodos de salida en problemas de clasificación es <b>Softmax</b>, la cual devuelve una probabilidad entre 0 y 1. Permite interpretar el resultado como un mapa de probabilidades sobre todas las clases, en lugar de una salida categórica. Esto puede ser útil en clasificación de muchas etiquetas, sobre todo cuando hay alta incerteza en la clasificación. \n",
    "\n",
    "¿Cuál de estas funciones es mejor? dependerá del problema, así que muchas veces querremos probar alternativas en nuestras capas de salida. \n",
    "\n",
    "Todas las anteriores sirven en capas de salida ante problemas de clasificación. Las que mencionaremos a continuación no se usan en capas de salida en problemas de clasificación porque se comportan como variables continuas y por tanto no pueden representar bien variables categóricas. \n",
    "    \n",
    "Para las capas ocultas ya hemos usado <b>Relu (Rectified Linear Unit)</b>. Básicamente es una función lineal pero que sólo se activa para valores positivos. \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:828/format:webp/1*XxxiA0jJvPrHEJHD4z893g.png\n",
    "          width=\"500\"/>\n",
    "Esto permite simular el comportamiento de una neurona que, ante cierto umbral se activa, pero que además puede transmitir en toda su intensidad la señal que recibe, en lugar de quedar limitada a un resultado binario. Así, la cantidad de información que puede comunicar es mucho mayor.\n",
    "    \n",
    "De hecho también es legítimo usar una activación <b>Lineal</b>, que puede ser simplemente la identidad. \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:828/format:webp/1*tldIgyDQWqm-sMwP7m3Bww.png\n",
    "          width=\"300\"/>\n",
    "\n",
    "Se puede probar muchas otras funciones de activación, por ejemplo una función <b>Exponencial</b> tendrá el efecto de activarse lento hasta un punto de saturación en el que la intensidad de salida se dispara. Muchas de las funciones de nuestro organismo presentan ese comportamiento, por ejemplo en nuestra percepción de temperatura. \n",
    "<br> <img src=https://www.researchgate.net/profile/Alexander-Bismark/publication/339240163/figure/fig83/AS:880668561117185@1586979155257/Figure-B6-Plot-of-the-Exponential-activation-function-For-a-definition-of-its.ppm\n",
    "          width=\"350\"/>\n",
    "   \n",
    "Casi siempre vamos a usar más de una función de activación en nuestras ANN, sobre todo para discriminar capas ocultas respecto a las de salida. Como siempre, no debemos suponer que se puede elegir con certeza la mejor combinación de funciones de activación. Parte importante de lo que se denomina <i>Aprendizaje Profundo</i> se logra acumulando muchas capas con distintas topologías y funciones de activación. \n",
    "    \n",
    "Sin embargo, siendo tanta la cantidad de alternativas (acá sólo se ha mencionado algunas), tampoco podemos buscar exhaustivamente por todas ellas. La recomendación es identificar casos donde problemas similares se hayan resuelto con redes neuronales y qué configuraciones fueron más exitosas, para intentar buscar desde allí. \n",
    "\n",
    "Probemos a continuación nuestra red con distintas funciones de activación en su capa de salida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f3ccb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 90   0   1   0   1   3   5   0   0   1]\n",
      " [  0 102   4   1   0   1   0   0   1   0]\n",
      " [  1   1  90   2   5   0   5   0   5   0]\n",
      " [  1   5   4  81   0   2   0   1   5   1]\n",
      " [  0   0   2   0  84   1   2   0   1   5]\n",
      " [  1   0   1   3   0  77   3   0   3   1]\n",
      " [  1   1   0   0   2   2 106   0   0   0]\n",
      " [  0   4   1   0   1   0   0  88   0   6]\n",
      " [  3   0   3  10   1   6   2   0  68   0]\n",
      " [  1   0   0   1   3   3   0   2   1  81]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Definimos el modelo de tipo ANN\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "# ACÁ CAMBIAMOS LA FUNCIÓN DE ACTIVACIÓN DE LA CAPA DE SALIDA\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Le pasamos la mejor epoch\n",
    "model.fit(X_train, y_train, epochs=60,\n",
    "          batch_size=32,\n",
    "          verbose = 0)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "\n",
    "# El set de datos de testeo y el entrenamiento, en este momento, son vectores.\n",
    "# Para poder evaluar nos conviene más que vuelva a ser un campo numérico\n",
    "y_test = [list(a) for a in y_test]\n",
    "y_hat = [list(a) for a in y_hat]\n",
    "y_test = [ a.index(max(a)) for a in y_test]\n",
    "y_hat = [ a.index(max(a)) for a in y_hat]\n",
    "\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ad0e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 90   0   0   0   0   2   7   0   2   0]\n",
      " [  0 106   1   0   0   1   0   0   1   0]\n",
      " [  1   2  90   1   5   0   5   0   5   0]\n",
      " [  0   1   2  88   0   4   0   2   2   1]\n",
      " [  0   0   1   0  76   1   1   0   4  12]\n",
      " [  2   0   3   2   1  77   0   1   3   0]\n",
      " [  1   0   0   0   3   4 101   0   3   0]\n",
      " [  0   4   0   0   2   0   0  85   0   9]\n",
      " [  1   3   0   1   0   2   3   0  81   2]\n",
      " [  0   1   0   1   0   2   0   3   2  83]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Definimos el modelo de tipo ANN\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "# ACÁ PROBAMOS OTRA MÁS. \n",
    "model.add(Dense(units=10, activation='tanh'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Le pasamos la mejor epoch\n",
    "model.fit(X_train, y_train, epochs=60,\n",
    "          batch_size=32,\n",
    "          verbose = 0)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "\n",
    "# El set de datos de testeo y el entrenamiento, en este momento, son vectores.\n",
    "# Para poder evaluar nos conviene más que vuelva a ser un campo numérico\n",
    "y_test = [list(a) for a in y_test]\n",
    "y_hat = [list(a) for a in y_hat]\n",
    "y_test = [ a.index(max(a)) for a in y_test]\n",
    "y_hat = [ a.index(max(a)) for a in y_hat]\n",
    "\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39325e",
   "metadata": {},
   "source": [
    "Observamos de los experimentos previos que Tanh dio resultados levemente mejores que Sigmoid y Softmax, en la capa de salida. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e66ec",
   "metadata": {},
   "source": [
    "<b>¿CÓMO APRENDE LA RED NEURONAL?</b>\n",
    "\n",
    "Cada nodo se activa según su función de activación y la señal que recibe. Que la señal sea mayor o menor que cero, por ejemplo, dependerá sobre todo de los pesos $w$ que estén configurados. Estos pesos $w$ son parámetros internos de la red.\n",
    "\n",
    "<b>NOTA: ¿Cuál es la diferencia entre parámetros e hiperparámetros?</b>  Todos los modelos se basan en variables internas que según el valor que adopten dan un resultado u otro, todas estas variables son parámetros del modelo. Aquellos parámetros que deben ser configurados manualmente por el investigador son denominados hiperparámetros, mientras que los valores que se ajustan automáticamente durante el entrenamiento no son hiperparámetros pero sí califican como parámetros. \n",
    "\n",
    "Los pesos $w$ son parámetros de la red neuronal, no los selecciona el investigador, se ajustan durante el entrenamiento. ¿Pero cómo se llega a establecer este ajuste? En un principio se asignan valores aleatorios, pero a continuación se entrena la red mostrándole un vector $x$ que representa los atributos de una observación que ingresa como entrada, y se computa una función de costo, error o perdida (tiene varios nombres) con el resultado a la salida, al compararlo con la clase real. ¿Cómo ocurre este proceso?\n",
    "\n",
    "<b><i>Forward Propagation</i></b>\n",
    "\n",
    "Sabemos que nuestra ANN es una secuencia de capas. Hemos visto que la entrada de una capa es la salida de la capa anterior. Así, una señal que ingresa por la capa de entrada, es transformada sucesivamente, capa por capa, hasta salir por la capa de salida. Este mecanismo se llama <b><i>Forward Propagation</i></b> (propagación hacia adelante). \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:828/format:webp/1*ddDKxWSAYci2dHaG3O_DOg.png\n",
    "          width=\"400\"/></br>\n",
    "\n",
    "¿Qué transformación se produce capa por capa? Por cada nodo ocurre una transformación lineal de sus datos de entrada, que depende del valor que tienen los pesos $w$ de la misma forma que ocurría con los parámetros $\\beta$ en la regresión lineal. A continuación se aplica una función de paso (función de activación).\n",
    "<br> <img src=https://images.deepai.org/glossary-terms/perceptron-6168423.jpg\n",
    "          width=\"400\"/>\n",
    "\n",
    "¿Cómo se establece la magnitud de cada peso $w$? eso es justamente lo que ocurre durante el entrenamiento, los pesos $w$ se ajustan hasta que logran generar buenas predicciones. \n",
    "\n",
    "<b><i>Gradient Descent</i></b>\n",
    "\n",
    "Lo primero que necesitamos es definir una función de costo asociada al error observado entre predicción y realidad. El entrenamiento de la ANN es un proceso de optimización que consiste en minimizar dicho costo ajustando los pesos $w$. Esto implica que entendemos que el costo es una función de los pesos $w$, por lo que podemos usar técnicas de optimización para encontrar un costo mínimo. El método del gradiente descendente, <b><i>Gradient Descent</i></b>, se basa en medir la dirección y magnitud de la pendiente de la curva de costo respecto a los pesos $w$, para lo cual usa las derivadas parciales entre costo y los $w$. A dicho vector de derivadas parciales se le llama <i>gradiente</i>, lo podemos imaginar como la dirección de caída de la superficie que forma la función de costo.  \n",
    "\n",
    "$$w = w - \\alpha \\frac{{\\partial error}}{{\\partial w}}$$\n",
    "\n",
    "<br> <img src=https://easyai.tech/wp-content/uploads/2019/01/tiduxiajiang-1.png\n",
    "          width=\"400\"/>\n",
    "          \n",
    "En la ecuación interviene también un hiperparámetro $\\alpha$ denominado tasa de aprendizaje (learning rate), valor que controla la magnitud de la actualización. Necesitamos que en cada iteración los pesos se actualicen en la dirección del mínimo de la función de costo, también necesitamos que ese avance converja al óptimo dentro del tiempo de entrenamiento, pero además debemos cuidar no ir tan rápido que en una iteración nos pasemos del mínimo al punto de quedar peor. En este último escenario lo que ocurriría es que la optimización no convergería a un óptimo, si no que sería divergente. La learning rate permite controlar la magnitud de cada salto, permitiendo buscar una mejor convergencia. \n",
    "<br> <img src=https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png\n",
    "          width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4a85f",
   "metadata": {},
   "source": [
    "<b><i>Backward Propagation</i></b>\n",
    "\n",
    "El problema es que no tenemos una forma de calcular directamente las derivadas parciales de la función de costo respecto a los pesos $w$. Pero, la función de costo es una función que representa el error del modelo, y por tanto depende de las etiquetas reales y predichas, es decir de $y$ e $\\hat{y}$. La función de costos la definimos nosotros, es la forma en la que decidamos calcular el error del modelo predictivo. Por lo tanto, al menos sí conocemos la derivada de la función de costo respecto a $\\hat{y}$.\n",
    "\n",
    "Por ejemplo, ya hemos usado el Error Cuadrático Medio cuando vimos Regresión Lineal, y en ese caso cada observación genera un error cuadrático de la forma:\n",
    "\n",
    "$$loss = error = (y - \\hat{y})^2$$\n",
    "\n",
    "Por lo que la derivada parcial tendrá la forma:\n",
    "\n",
    "$$\\frac{\\partial error}{\\partial \\hat{y}} = 2 (\\hat{y} - y)$$\n",
    "\n",
    "Desde esta derivada parcial podemos llegar a las que corresponden al error (función de costo) respecto de los $w$ de cada capa, gracias al uso de la regla de la cadena en el método que se denomina <b><i>Backward Propagation</i></b>. \n",
    "\n",
    "La salida $\\hat{y}$ es función de los pesos de la última capa, por lo que podemos calcular la derivada de la salida $\\hat{y}$ respecto a los pesos $w$. Usando la regla de la cadena obtenemos la derivada de la función de costo respecto a los pesos $w$. \n",
    "\n",
    "$$ \\frac{\\partial error}{\\partial w} = \\frac{\\partial error}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w}$$\n",
    "\n",
    "Así que, lo que hemos logrado con lo anterior es relacionar el gradiente del costo respecto a los pesos $w$ de la última capa con el gradiente de la predicción $\\hat{y}$ respecto a dichos pesos. Así que necesitamos la gradiente de la entrada de la capa anterior, que a su vez es la salida de la capa anterior a la anterior... O sea, se repite todo el análisis pero una capa más atrás. \n",
    "\n",
    "La propagación en reversa se repite de capa en capa, desde la última a la primera capa de la red. En dicha primera capa la entrada no es otra cosa que $x$, es decir el vector de entrada que representa cada observación del set de datos. De acá el nombre del método <b><i>backward propagation</i></b>, propagación hacia atrás, puesto que la magnitud del ajuste de pesos se computa desde la salida de la ANN hacia la entrada. \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:828/format:webp/1*0QPRST83oBicKPE_R4biJA.png\n",
    "          width=\"600\"/>\n",
    "   \n",
    "\n",
    "<br> <img src=https://ih1.redbubble.net/image.5142581850.1941/bg,f8f8f8-flat,750x,075,f-pad,750x1000,f8f8f8.jpg\n",
    "          width=\"200\"/>\n",
    "¿Hasta dónde es importante entender el proceso de aprendizaje de la ANN?\n",
    "\n",
    "Las ciencias de la computación, así como las ciencias de datos, aprovechan los recursos que se generan a partir de descubrimientos como el descrito. La buena noticia es que los ingenieros trabajamos con los artefactos que se han construido de ellos, no con el marco teórico fundamental. Sin embargo, ello no significa que no requiramos una comprensión general de los procesos involucrados en el funcionamiento de las herramientas de las que dependemos. \n",
    "\n",
    "Queda pendiente para asignatura electiva hacer mayor demostración del gradiente descedente, así como programar una red neuronal <i>from the scratch</i>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a88c74",
   "metadata": {},
   "source": [
    "En los experimentos previos usamos como Optimizador <b>Adam (Adaptive Moment Estimation)</b>, el cual es un algoritmo nuevo (2015), pero que se basa en buena parte en el clásico Gradiente Descendente (que viene del siglo XIX). Todavía podemos usar el algoritmo clásico, pero las versiones modernas son más eficiente y rápidas para llegar al óptimo. Adam en particular es considerado el más popular en entrenamiento de ANNs profundas, de modo que no tenemos grandes motivaciones para cambiarlo. \n",
    "\n",
    "<b>Hiperparámetros de ADAM</b>\n",
    "\n",
    "Ya hemos hablado del <b>Learning rate</b> que multiplica al gradiente y controla la velocidad del aprendizaje. Su valor predeterminado es es 0.001, se puede aumentar para acelerar el aprendizaje, aunque si se nos pasa la mano terminaremos con una red que diverge en lugar de converger a un óptimo. \n",
    "\n",
    "Adam agrega el hiperparámetro <b>decay</b>, que especifica la tasa de decaimiento para el learning rate en cada actualización de los pesos $w$. Si se establece un valor distinto de cero, el learning rate disminuye gradualmente a medida que avanza el entrenamiento. Esto puede ayudar a estabilizar el proceso de optimización y evitar fluctuaciones bruscas en los pesos. \n",
    "\n",
    "Adam considera otros hiperparámetros, dos de ellos que suele ser útil explorar son <b>beta_1</b> y <b>beta_2</b>. Ambos son coeficientes de decaimiento para los momentos de primer y segundo orden en el optimizador Adam. Controlan la contribución relativa de los momentos pasados en el cálculo de los momentos actuales. Los valores predeterminados son 0.9 y 0.999 respectivamente.\n",
    "\n",
    "Estos y otros hiperparámetros del optimizador permiten controlar la velocidad y capacidad para converger del algoritmo que encuentra el óptimo. También facilitan al algoritmo evitar caer en óptimos locales \n",
    "\n",
    "<br> <img src=\"https://www.allaboutlean.com/wp-content/uploads/2018/08/Local-Global-Optimum.png\"\n",
    "          width=\"400\"/>\n",
    "          \n",
    "A continuación probaremos algunos hiperparámetros del optimizador. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e04cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[100   0   0   0   0   0   1   0   0   0]\n",
      " [  0 101   1   2   0   0   1   1   3   0]\n",
      " [  1   2  91   2   4   0   2   1   6   0]\n",
      " [  1   5   2  86   0   0   1   1   4   0]\n",
      " [  1   0   1   2  85   0   3   0   3   0]\n",
      " [ 17   5   1  35   7   0   7   0  17   0]\n",
      " [  3   0   2   0   2   0 102   0   3   0]\n",
      " [  0   2   0   1   5   0   0  89   3   0]\n",
      " [  1   2   2   5   1   0   3   0  79   0]\n",
      " [  2   1   0   2  39   0   0  30  18   0]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Definimos el modelo de tipo ANN\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=10, activation='tanh'))\n",
    "\n",
    "# DECLARAMOS EL OPTIMIZADOR PARA CONFIGURAR SUS HIPERPARAMETROS\n",
    "# Ello va a incluir un programa de decaimiento del Learning Rate. \n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.9)\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Y LO PASAMOS AL COMPILADOR\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# DADO QUE ESTAMOS USANDO DECAY, AUMENTAREMOS LOS EPOCH PARA COMPENSAR\n",
    "model.fit(X_train, y_train, epochs=120,\n",
    "          batch_size=32,\n",
    "          verbose = 0)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "\n",
    "# El set de datos de testeo y el entrenamiento, en este momento, son vectores.\n",
    "# Para poder evaluar nos conviene más que vuelva a ser un campo numérico\n",
    "y_test = [list(a) for a in y_test]\n",
    "y_hat = [list(a) for a in y_hat]\n",
    "y_test = [ a.index(max(a)) for a in y_test]\n",
    "y_hat = [ a.index(max(a)) for a in y_hat]\n",
    "\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6baabcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 94   0   1   0   0   2   3   0   1   0]\n",
      " [  0 104   1   0   0   1   1   0   2   0]\n",
      " [  2   1  91   2   6   0   0   2   5   0]\n",
      " [  1   1   3  83   0   2   2   1   7   0]\n",
      " [  0   1   1   0  83   2   1   0   1   6]\n",
      " [  2   1   0   2   1  80   0   0   2   1]\n",
      " [  0   1   0   0   1   5 105   0   0   0]\n",
      " [  0   2   0   1   2   0   0  88   0   7]\n",
      " [  1   3   2   2   1   7   2   0  74   1]\n",
      " [  1   1   0   0   2   2   0   1   3  82]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.884\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Definimos el modelo de tipo ANN\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=10, activation='tanh'))\n",
    "\n",
    "# DECLARAMOS EL OPTIMIZADOR PARA CONFIGURAR SUS HIPERPARAMETROS\n",
    "# Buscamos una mejora configuracion de hiperparámetros del optimizador. \n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.5)\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Y LO PASAMOS AL COMPILADOR\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# DADO QUE ESTAMOS USANDO DECAY, AUMENTAREMOS LOS EPOCH PARA COMPENSAR\n",
    "model.fit(X_train, y_train, epochs=120,\n",
    "          batch_size=32,\n",
    "          verbose = 0)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "\n",
    "# El set de datos de testeo y el entrenamiento, en este momento, son vectores.\n",
    "# Para poder evaluar nos conviene más que vuelva a ser un campo numérico\n",
    "y_test = [list(a) for a in y_test]\n",
    "y_hat = [list(a) for a in y_hat]\n",
    "y_test = [ a.index(max(a)) for a in y_test]\n",
    "y_hat = [ a.index(max(a)) for a in y_hat]\n",
    "\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aef195",
   "metadata": {},
   "source": [
    "Algo hemos mejorado los resultados ajustando hiperparámetros del optimizador, pero debiéramos organizar una búsqueda exhaustiva de la configuración idónea, usando muestra de validación o validación cruzada, si tuviéramos más tiempo para ello. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd096541",
   "metadata": {},
   "source": [
    "<b>REDES NEURONALES PARA REGRESIÓN</b>\n",
    "\n",
    "Si queremos que nuestra red neuronal no clasifique entre categorías, y en cambio devuelva estimación de una variable numérica, basta con reducir la capa de salida a un único nodo y poner allí una activación lineal (Relu serviría para valores que son siempre positivos). Además, tendremos que cambiar la función de pérdida para medir el error de una regresión en lugar de clasificación, por lo general usaríamos el error cuadrático medio. \n",
    "\n",
    "Probemos esta idea con el problema de la Planta de Ciclo Combinado. Ya habíamos usado este set de datos para ilustrar Regresión Lineal, pero en ese entonces no teníamos el cuidado de comprobar nuestros resultados con una muestra de testeo. Repitamos el experimento de la regresión lineal y veamos que tan bien generaliza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db34bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importar datos desde csv\n",
    "file = open(\"Ciclo_Combinado.csv\", \"r\")\n",
    "obs = file.read()\n",
    "file.close()\n",
    "obs = obs.split(\"\\n\")\n",
    "# hay un salto de línea demás en el archivo\n",
    "obs.remove(obs[-1])\n",
    "# dividir los datos por coma. \n",
    "for j in range(len(obs)):\n",
    "    obs[j] = obs[j].split(\",\")\n",
    "# transformar a numpy array.\n",
    "obs = np.array(obs)\n",
    "# Valores Y (variable dependiente)\n",
    "y = obs[:,-1][1:]\n",
    "# Matriz X (variables independientes)\n",
    "X = obs[:,:-1][1:]\n",
    "\n",
    "# Los valores de X e y tienen que ser numéricos\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "\n",
    "# Usamos Min-Max para estandarizar el rango de datos \n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Usaremos 80% para entrenar y 20% para testear\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4961dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente de determinación (R2): 0.9270312819720488\n",
      "Intercepto (Beta0): 502.6779548850371\n",
      "Pendiente (Betas 1 en adelante): [-69.67963929 -13.19868804   2.52774185 -11.85391564]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Se define el modelo\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "r_sq = model.score(X_test, y_test)\n",
    "print('Coeficiente de determinación (R2):', r_sq)\n",
    "print('Intercepto (Beta0):', model.intercept_)\n",
    "print('Pendiente (Betas 1 en adelante):', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55772a49",
   "metadata": {},
   "source": [
    "<b>Métricas para evaluar Regresión</b>\n",
    "\n",
    "El <b>Error Cuadrático Medio</b> es una de las métricas más relevantes en los problemas de Regresión, el método de optimización de Regresión Lineal se basa en la minimización de esta métrica. Ya hemos mencionado que podemos usarlo como función de perdida en una red neuronal. Por lo tanto, podemos evaluar los resultados de la regresión usando el error cuadrático medio (MSE). \n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "MSE nos da el error medio al cuadrado, por lo que sacar su raíz nos permite tener una idea más clara del error medio que produce el modelo, en la misma escala que la variable $y$. \n",
    "\n",
    "El score con que se evalúa por defecto en Regresión Lineal es el <b>coeficiente de determinación $R^{2}$</b>. Este mide cuanto de la variabilidad de los datos reales logra ser explicada por el modelo, devolviendo un rango de 0 a 1 donde 1 correspondería a una determinación perfecta entre modelo y observaciones, 0 corresponde a que el modelo no dice nada respecto a las observaciones (o al menos que dice menos de lo que ya dice la media). Se obtiene de la división del error cuadrático con la desviación respecto a la media al cuadrado. \n",
    "\n",
    "$$R^2 = 1 - \\frac{{\\sum_i (y_i - \\hat{y}_i)^2}}{{\\sum_i (y_i - \\bar{y})^2}}$$\n",
    "\n",
    "Podría darse el caso de un coeficiente de determinación $R^{2}$ negativo, pero sería la situación donde el modelo no sólo no explica las observaciones, sino que es consistentemente peor que asumir el valor promedio como predicción. Esta es una situación anómala, por lo que el rango de $R^{2}$ se considera entre 0 y 1. \n",
    "\n",
    "Respecto al problema de la planta de ciclo combinado, con Regresión Lineal, al entrenar el modelo ya obtuvimos el coeficiente de determinación $R^{2}$ sobre los datos de entrenamiento. Evaluemos el MSE  y el coeficiente de determinación sobre la muestra de testeo y comparemos respecto a la predicción de entrenamiento. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dce7c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cuadrático Medio de entrenamiento: 20.59326659435902\n",
      "Raíz del Error Cuadrático Medio de entrenamiento: 4.537980453280844\n",
      "Coeficiente de Determinación R2 de entrenamiento: 0.9291053443563475\n",
      "\n",
      "Error Cuadrático Medio de testeo: 21.468390968412617\n",
      "Raíz del Error Cuadrático Medio de testeo: 4.633399504512061\n",
      "Coeficiente de Determinación R2 de testeo: 0.9270312819720488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Resultado sobre el entrenamiento\n",
    "y_hat = model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_hat)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_hat)\n",
    "print(\"Error Cuadrático Medio de entrenamiento:\", mse)\n",
    "print(\"Raíz del Error Cuadrático Medio de entrenamiento:\", rmse)\n",
    "print(\"Coeficiente de Determinación R2 de entrenamiento:\", r2)\n",
    "print()\n",
    "# y con sombrero son los resultados de la predicción\n",
    "y_hat = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_hat)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_hat)\n",
    "print(\"Error Cuadrático Medio de testeo:\", mse)\n",
    "print(\"Raíz del Error Cuadrático Medio de testeo:\", rmse)\n",
    "print(\"Coeficiente de Determinación R2 de testeo:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec3b6f",
   "metadata": {},
   "source": [
    "Vemos que con Regresión Lineal se da el patrón habitual en que los resultados de testeo se ven peor que los de entrenamiento, pero la diferencia es bastante poca. Regresión Lineal no es un modelo que pueda sobre ajustar mucho, dada la naturaleza lineal del mismo. \n",
    "\n",
    "¿Podemos lograr un mejor modelo predictivo de la potencia que genera la planta usando una Red Neuronal? Veamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37bfa23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Definimos el modelo de tipo ANN\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "# ACÁ CAMBIAMOS LA FORMA DE LA SALIA A UN NODO CON ACTIVACIÓN LINEAL\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# ACÁ CAMBIAMOS LA FUNCIÓN DE PERDIDA POR MSE. \n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Entrenamos 60 épocas\n",
    "model.fit(X_train, y_train, epochs=60,\n",
    "          batch_size=32,\n",
    "          verbose = 0)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# la predicción es una lista de vectores con un elemento, necesitamos que sean números\n",
    "y_hat = [a[0] for a in y_hat]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ba63ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Resultado Real  Resultado Predicho\n",
      "0          442.70          430.892975\n",
      "1          445.95          476.838898\n",
      "2          432.09          448.179321\n",
      "3          427.05          450.435394\n",
      "4          438.28          440.231812\n",
      "5          458.19          446.907837\n",
      "6          435.94          428.565125\n",
      "7          442.49          429.447144\n",
      "8          479.48          469.028168\n",
      "9          467.22          477.866486\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# imprimieromos sólo una muestra de casos al azar. \n",
    "samp = random.sample(range(len(y_test)),10)\n",
    "\n",
    "# Crear un diccionario con los resultados reales y predichos para la muestra\n",
    "data = {\n",
    "    'Resultado Real': [y[i] for i in samp],\n",
    "    'Resultado Predicho': [y_hat[i] for i in samp]\n",
    "}\n",
    "\n",
    "# Convertir el diccionario a un DataFrame de pandas para mostrar en tabla\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar la tabla\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8608b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 899us/step\n",
      "Error Cuadrático Medio de entrenamiento: 19.35311515881083\n",
      "Raíz del Error Cuadrático Medio de entrenamiento: 4.399217562113839\n",
      "Coeficiente de Determinación R2 de entrenamiento: 0.933374706313389\n",
      "\n",
      "60/60 [==============================] - 0s 985us/step\n",
      "Error Cuadrático Medio de testeo: 20.328536292488383\n",
      "Raíz del Error Cuadrático Medio de testeo: 4.508717810252532\n",
      "Coeficiente de Determinación R2 de testeo: 0.9309055236216783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Resultado sobre el entrenamiento\n",
    "y_hat = model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_hat)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_hat)\n",
    "print(\"Error Cuadrático Medio de entrenamiento:\", mse)\n",
    "print(\"Raíz del Error Cuadrático Medio de entrenamiento:\", rmse)\n",
    "print(\"Coeficiente de Determinación R2 de entrenamiento:\", r2)\n",
    "print()\n",
    "# y con sombrero son los resultados de la predicción\n",
    "y_hat = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_hat)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_hat)\n",
    "print(\"Error Cuadrático Medio de testeo:\", mse)\n",
    "print(\"Raíz del Error Cuadrático Medio de testeo:\", rmse)\n",
    "print(\"Coeficiente de Determinación R2 de testeo:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a2e0ad",
   "metadata": {},
   "source": [
    "Obtenemos una mejor predicción con la ANN que la que nos daba Regresión Lineal. Puede que parezca una pequeña diferencia, pero ese 0.12 de error que en promedio estamos reduciendo en cada predicción (que bajó de 4.63 a 4.51 al cambiar de Regresión Lineal a Red Neuronal), podría tener un impacto económico importante. Además, no hemos realizado aún ningún esfuerzo por optimizar la Red Neuronal, así que bien podríamos esperar que mejore si incrementamos la topología, buscamos el entrenamiento adecuado, probamos otras funciones de activación y/o probamos cambiar la función de costo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1223a",
   "metadata": {},
   "source": [
    "<b> CONCLUSIÓN </b>\n",
    "\n",
    "Hemos profundizado en la implementación de modelos de clasificación y regresión usando Redes Neuronales Artificiales. Se ha buscado aportar a la comprensión del funcionamiento del algoritmo de optimización clásico, Gradiente Descendente, aunque queda mucho tema por desarrollar allí y este es un curso más práctico que teórico. Se ha ilustrado las similitudes y diferencias que ocurren al abordar problemas de clasificación y regresión con redes neuronales.\n",
    "\n",
    "Hemos podido constatar que las redes neuronales artificiales, sin grandes adaptaciones, sirven en problemas de clasificación como de regresión. Ello da cuenta de la universalidad que caracteriza este tipo de modelo de aprendizaje, lo cual junto a sus muchas otras virtudes le han valido transformarse en un importante pilar de la Ciencia de Datos. \n",
    "\n",
    "El apunte aborda los temas teóricos que deben ser estudiados en la asignatura, a la vez que provee de material práctico en la forma de código en Python y uso de librerías y recursos. No reemplaza la búsqueda y curiosidad de quién está aprendiendo, pero es una guía sobre la cual se basará las futuras evaluaciones. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
