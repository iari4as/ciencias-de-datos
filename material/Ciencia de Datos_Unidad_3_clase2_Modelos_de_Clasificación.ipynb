{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26092db2",
   "metadata": {},
   "source": [
    "<head><b><center>CIENCIA DE DATOS</center>\n",
    "\n",
    "<center>APRENDIZAJE SUPERVISADO</center>\n",
    "<center>Profundizando en Clasificación</center></b>\n",
    "    \n",
    "\n",
    "<center>Profesor: Gabriel Jara </center></head><br>\n",
    "El presente Jupyter Notebook busca:\n",
    "<ul>\n",
    "    <li>Reforzar conceptos presentados previamente sobre problemas de clasificación.</li>\n",
    "    <li>Avanzar nuestro dominio sobre el uso de Árboles de Decisión en clasificación.</li>\n",
    "    <li>Explicar cómo se busca mejores configuraciones de hiperparámetros.</li>\n",
    "    <li>Explorar otros algoritmos para clasificación: KNN, SVM, ANN.</li>\n",
    "  <li>Incrementar nuestro conocimiento sobre herramientas disponibles en Python para la Ciencia de Datos.</li>\n",
    "</ul>\n",
    "\n",
    "Nota: Las imágenes que han sido robadas de internet son enlaces a su correspondiente fuente. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d672aa9",
   "metadata": {},
   "source": [
    "<b>En el capítulo anterior...</b>\n",
    "\n",
    "En el capítulo anterior estudiamos los dos problemas fundamentales para los que se usa el aprendizaje supervisado: <b>REGRESIÓN</b> y <b>CLASIFICACIÓN</b>\n",
    "\n",
    "<br> <img src=https://www.simplilearn.com/ice9/free_resources_article_thumb/Regression_vs_Classification.jpg width=\"400\"/>\n",
    "\n",
    "Primero obsrvamos como se resuelven usando modelos lineales: <b>Regresión lineal</b> y <b>Regresión Logística</b>. Luego trabajamos con modelos de mayor capacidad de representación del tipo <b>Árbol de Decisión</b>, logrando ajustar mejor a la forma de los datos.  Pero ello nos lleva a encontrar un nuevo problema: <b>SOBREAJUSTE (<i>OVERFITTING</i>)</b>.\n",
    "\n",
    "<br> <img src=https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/overfitting_2.png width=\"400\"/>\n",
    "\n",
    "\n",
    "De lo anterior nace la necesidad de dividir la muestra en <b>ENTRENAMIENTO</b>, <b>VALIDACIÓN</b> y <b>TESTEO</b>. \n",
    "\n",
    "<br> <img src=https://upload.wikimedia.org/wikipedia/commons/b/bb/ML_dataset_training_validation_test_sets.png width=\"400\"/>\n",
    "\n",
    "O bien, utilizar estrategia de <b>VALIDACIÓN CRUZADA</b> para no tener que tener una muestra aparte para validación, aunque sí debieramos reservar la de testeo. \n",
    "\n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:1200/1*AAwIlHM8TpAVe4l2FihNUQ.png width=\"400\"/></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfae817",
   "metadata": {},
   "source": [
    "<b>ÁRBOL DE DECISION PARA CLASIFICACIÓN</b>\n",
    "\n",
    "En el contexto de la clasificación, un Árbol de Decisión toma un conjunto de características de entrada y las usa para tomar una decisión \"jerárquica\" y clasificar una nueva entrada en una de varias clases posibles. Durante la clase pasada vimos que podemos entrenar estas estas estructuras a partir de las obsvervaciones que ya tengamos clasificadas, para usarlas luego en predecir la clasificación de nuevas observaciones.\n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:781/1*fGX0_gacojVa6-njlCrWZw.png width=\"600\"/>\n",
    "\n",
    "Con todo ello logramos ajustar mejor el Árbol de Decisión para clasificación en el problema del Titanic, jugando sólo con la profundidad del árbol. Pero hemos dejado sin explorar buena parte de las posibilidades que los árboles de decisión ofrecen.\n",
    "<br> <img src=https://pbs.twimg.com/media/ETBrXfgXkAAIFGp.jpg width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2485a33",
   "metadata": {},
   "source": [
    "<b>CRITERIO DE DIVISIÓN</b>\n",
    "\n",
    "Hemos señalado que durante la construcción del árbol para cada nodo se elige el atributo que mejor separa las clases, pero ¿qué significa en concreto? Se necesita alguna métrica sobre cuánto separa las clases cada atributo. Esta medida de separación debe ser una función que podamos calcular, a partir de los datos y que nos sirva de criterio para elegir el atributo que mejor nos acerca a tener grupos de observaciones limpias, es decir que agrupan una única clase. \n",
    "\n",
    "En nuestro experimento con el Titanic estamos usando DecisionTreeClassifier del módulo sklearn.tree, y por ahora sólo hemos usado el hiperparámetro max_depth para modificar la profundidad del árbol. Otro hiperparámetro se denomina <b><i>criterion</i></b>, y sirve justamente para establecer la función que se usará de criterio para evaluar los hiperparámetros que se usen para dividir los datos en cada nodo del árbol. Por defecto usa el <b>índice Gini</b>, que es lo que ya podíamos observar en los plots de árboles que hicimos desde la clase pasada. \n",
    "\n",
    "<b>Índice Gini</b>\n",
    "\n",
    "El índice de Gini mide la probabilidad de que dos instancias seleccionadas al azar de un conjunto de datos pertenezcan a clases diferentes. Un valor de Gini de 0 indica pureza total (es decir, todas las observaciones pertenecen a la misma clase), mientras que un valor más cercano a 1 indica mayor impureza (es decir, hay una distribución más uniforme entre las clases).\n",
    "\n",
    "El índice de Gini se define como:\n",
    "\n",
    "$$Gini(S) = 1 - \\sum_{i=1}^{c} (P_i)^2$$\n",
    "\n",
    "Donde $S$ es el conjunto de datos, $c$ es el número de clases y $P_i$ es la probabilidad de que un ejemplo pertenezca a la clase $i$. Para calcular el índice de Gini primero debemos calcular la probabilidad de cada clase.\n",
    "\n",
    "Supongamos que tenemos un conjunto de datos con tres clases y 10 ejemplos en total, de acuerdo a la siguiente distribución:\n",
    "<ul>\n",
    "    <li>Clase A: 6 observaciones.</li>\n",
    "    <li>Clase B: 3 observaciones.</li>\n",
    "    <li>Clase C: 1 observación.</li>    \n",
    "</ul>\n",
    "La probabilidad de cada clase es la proporción de casos que pertenecen a esa clase en el conjunto de datos:\n",
    "<ul>\n",
    "    <li>Clase A: $6/10 = 0.6$</li>\n",
    "    <li>Clase B: $3/10 = 0.3$</li>\n",
    "    <li>Clase C: $1/10 = 0.1$</li>    \n",
    "</ul>\n",
    "Ahora calculamos el índice de Gini siguiendo la fórmula. \n",
    "\n",
    "$$Gini = 1-(0.6^2 + 0.3^2 + 0.1^2) = 0.54$$\n",
    "\n",
    "Por lo tanto, el índice de Gini para este conjunto de datos es 0.54. \n",
    "\n",
    "El índice de Gini se mueve en un rango general de 0 a 1. Sin embargo, el límite superior queda acotado según cuantos distintos valores existan como clase. Si sólo hay dos clases (problema de clasificación binario) el índice de Gini tiene un valor máximo de 0.5. \n",
    "\n",
    "Los algoritmos que implementan árboles de decisión van recorriendo los atributos disponibles, seleccionando distintas potenciales divisiones y calculando los índices de Gini de los nodos hijo que se generan. Entre más bajo es el índice Gini para un nodo, significa que está compuesto por observaciones más puras (de una misma clase), y por lo tanto se selecciona el atributo que consiguió índices Gini más bajos. Un nodo cuyo índice Gini es cero ya está compuesto por una única clase y debiera ser dejado como nodo terminal (hoja). \n",
    "\n",
    "Pero, además de Gini, el mismo proceso se puede lograr usando otras métricas que capturen la variabilidad de clases que se va generando en cada partición, en cada nodo. \n",
    "\n",
    "<b>Entropía</b>\n",
    "\n",
    "La Entropía (Entropy), también conocida como Entropía de Shannon, es otra métrica que también se basa en las proporciones de observaciones de cada clase, auque con un cálculo e interpretación distinto. Mide la incertidumbre promedio asociada a una variable aleatoria.    \n",
    "\n",
    "$$H(S) = -\\sum_{i=1}^{c} P_i \\log_2 ( P_i )$$\n",
    "\n",
    "Donde $S$ es el conjunto de datos, $c$ es el número de clases y $P_i$ es la probabilidad de que un ejemplo pertenezca a la clase $i$, y $\\log_2$ se refiere a la función logaritmo en base 2. \n",
    "\n",
    "En la fórmula de entropía, el logaritmo en base 2 nos permite calcular cuántos bits se necesitan, en promedio, para identificar el valor de una variable aleatoria con una distribución de probabilidad dada. Al multiplicarla por la probabilidad de ocurrencia de la clase, obtenemos la esperanza de la información requerida para identificar la clase. Así, la entropía representa la esperanza de información requerida para identificar toda la muestra, medido en bits. \n",
    "\n",
    "Volvamos a suponer que las clases se dividen así:\n",
    "<ul>\n",
    "    <li>Clase A: 6 observaciones.</li>\n",
    "    <li>Clase B: 3 observaciones.</li>\n",
    "    <li>Clase C: 1 observaciones.</li>    \n",
    "</ul>\n",
    "La probabilidad de cada clase es la proporción de casos que pertenecen a esa clase en el conjunto de datos:\n",
    "<ul>\n",
    "    <li>Clase A: $6/10 = 0.6$</li>\n",
    "    <li>Clase B: $3/10 = 0.3$</li>\n",
    "    <li>Clase C: $1/10 = 0.1$</li>    \n",
    "</ul>\n",
    "La formula de la Entropía sería:\n",
    "\n",
    "$$H(S) = -(0.6*\\log_2 (0.6)+0.3\\log_2 (0.3)+0.1*\\log_2 (0.1)) = 1.23$$\n",
    "\n",
    "Interpretaríamos que, aunque sepamos que una observación está en este grupo, aún nos falta 1.23 bits de información para decir con certeza a qué clase pertenece. Pero si no supiéramos que la observación proviene de este grupo, no tuviéramos ningún antecedente sobre su clase y dado que hay 3 clases, necesitaríamos $\\log_2(3)=1.58$ bits para almacenar la información de su clase. \n",
    "\n",
    "En un problema de clasificación binario la información de la clase de una observación cabe en  $\\log_2(2)=1$ bit. Por lo tanto, la Entropía queda acotada a un máximo de 1 en ese tipo de problema. Cuando la entropía es 0, por otro lado, significa que no falta ninguna información adicional para clasificar una observación, y es el caso de que baste saber que llegó a un nodo para decir con certeza su clase, es decir que todo el grupo de observaciones que llega a ese nodo queda compuesto de sólo una única clase. \n",
    "\n",
    "Probemos a continuación el árbol de decisión para el Titanic, usando Entropía como criterio, en lugar de Gini. Para no complicar las cosas, dejemos la profundidad máxima en 2.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Función para cargar el CSV\n",
    "def cargar_datos(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# Función para eliminar columnas innecesarias\n",
    "def eliminar_columnas(df):\n",
    "    return df.drop(columns=['Name', 'Ticket', 'Cabin', 'Embarked'])\n",
    "\n",
    "# Función para transformar género a valores numéricos\n",
    "def codificar_sexo(df):\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    return df\n",
    "\n",
    "# Función para eliminar filas con valores faltantes\n",
    "def eliminar_nulos(df):\n",
    "    return df.dropna()\n",
    "\n",
    "# Función principal que aplica todo el pipeline y extrae X e y\n",
    "def preprocesar_titanic(path):\n",
    "    df = cargar_datos(path)\n",
    "    df = eliminar_columnas(df)\n",
    "    df = codificar_sexo(df)\n",
    "    df = eliminar_nulos(df)\n",
    "    \n",
    "    df = df.to_numpy()\n",
    "\n",
    "    # Separamos la columna 1 como y, y desde la columna 2 en adelante como X, excluyendo el encabezado\n",
    "    y = df[:,1]\n",
    "    X = df[:,2:]\n",
    "    X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Uso del pipeline\n",
    "X, y = preprocesar_titanic(\"Titanic.csv\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Dividimos en muestra de entrenamiento y teteo. \n",
    "# Por ahora no usaremos validación.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Creamos nuestro clasificador y lo entrenamos con muestra de entrenamiento. \n",
    "clf = DecisionTreeClassifier(random_state=1234, criterion='entropy', max_depth=2)\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualizamos el árbol de decisión para clasificar\n",
    "plt.figure(figsize=(400,100))\n",
    "tree.plot_tree(clf, feature_names=[\"clase\", \"género\", \"edad\", \n",
    "                                   \"conyugues-hermanos\", \"padres-hijos\" , \"tarifa\"], \n",
    "               class_names=[\"No sobrevice\", \"Sí sobrevive\"], filled=True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo con la muestra de testeo.\n",
    "y_hat = model.predict(X_test)\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "print('MATRIZ DE CONFUSIÓN para Árbol con Criterio Entropía y Profundidad máxima 2:')\n",
    "print(matriz_conf,'\\n')\n",
    "print('BIEN CLASIFICADOS: ', matriz_conf[0][0]+matriz_conf[1][1])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 0: ', matriz_conf[0][0])\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 1: ', matriz_conf[1][1])\n",
    "print('MAL CLASIFICADOS: ', matriz_conf[0][1]+matriz_conf[1][0])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 1: ', matriz_conf[0][1],'(falso positivo)')\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 0: ', matriz_conf[1][0],'(falso negativo)')\n",
    "print('La exactitud de testeo del modelo con profundidad {} es: {:.3f}'.format(2,model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d8439",
   "metadata": {},
   "source": [
    "<b><i>GRID SEARCH</i></b>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aba1c1",
   "metadata": {},
   "source": [
    "Hemos construido varios árboles modificando su profundidad máxima y criterio de división. Hay otros hiperparámetros que también podríamos probar, en la búsqueda del árbol que dé mejor resultados, en validación cruzada. Algunos de estos hiperparámetros son: \n",
    "\n",
    "<ul><li><b>min_samples_split</b> es el número mínimo de casos requeridos para dividir un nodo. Si se establece un valor muy bajo, el modelo puede ser propenso a sobreajustarse. Por otro lado, si se establece un valor muy alto, el modelo puede no capturar suficientes relaciones en los datos.</li>\n",
    "\n",
    "<li><b>min_samples_leaf</b> es el número mínimo de casos requeridos para ser un nodo hoja. Si se establece un valor muy bajo, el modelo puede sobreajustarse, mientras que si se establece un valor muy alto, el modelo puede no capturar suficientes detalles de los datos.</li>\n",
    "\n",
    "<li><b>ccp_alpha</b> controla la aplicación de <b><i>pruning</i></b> es decir que se poden ramas del árbol, siguiendo un criterio de complejidad. Valores altos en este hiperparámetro reducen la complejidad del árbol, lo que puede mejorar su capacidad para generalizar a nuevos datos. Por otro lado, un valor demasiado alto de ccp_alpha puede hacer que el modelo pierda información importante.</li></ul>\n",
    "\n",
    "¿Cómo podemos considerar todos los hiperparámetros que tenemos disponibles en una búsqueda sistemática? Básicamente tenemos dos opciones:\n",
    "\n",
    "<ul>\n",
    "    <li><b><i>Random Search</i></b>: realizar una búsqueda aleatoria y repetirla hasta que tengamos una alta probabilidad de tener buenos resultados.</li>\n",
    "    <li><b><i>Grid Search</i></b>: realizar una búsqueda exhaustiva sobre una cuadricula de combinaciones de los hiperparámetros disponibles</li></ul>\n",
    "    \n",
    "Para cada combinación de hiperparámetros se evalúa el modelo usando validación (set de validación o validación cruzada), al final se reporta la mejor configuración encontrada. \n",
    "\n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:1004/0*yDmmJmvRowl0cSN8.png width=\"400\"/>\n",
    "\n",
    "Hagamos a continuación un grid search sobre algunos hiperparámetros de nuestro modelo de árbol de clasificación. Por supuesto, al ser una estrategia de fuerza bruta, tiene su costo a pagar que es básicamente tiempo de cómputo. O sea, se va a demorar, y si el problema a resolver es grande, puede ser que se demore más de lo que resulte práctico. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Midamos el tiempo que toma nuestra búsqueda\n",
    "import time\n",
    "inicio = time.time()\n",
    "\n",
    "# Uso del pipeline\n",
    "X, y = preprocesar_titanic(\"Titanic.csv\")\n",
    "\n",
    "# Vamos a usar validación cruzada, así que sólo necesitamos apartar testeo.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Definimos el modelo de clasificación\n",
    "clf = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "# Definimos la grilla de hiperparámetros que vamos a probar\n",
    "param_grid = {'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,None],\n",
    "              'criterion': ['gini','entropy'],\n",
    "              'min_samples_split': [2,3,4,5,6,7,8],\n",
    "              'min_samples_leaf': [1,2,3,4,5,6],\n",
    "              'ccp_alpha': [0,0.0001,0.001,0.01,0.1,1]\n",
    "             }\n",
    "# Realizamos búsqueda de grilla con validación cruzada de 5 carpetas\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "# Ajustar el modelo con búsqueda de grilla\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimimos mejores hiperparámetros y mejor puntaje\n",
    "print(\"Mejores hiperparámetros: \", grid_search.best_params_)\n",
    "print(\"Mejor puntaje: \", grid_search.best_score_)\n",
    "\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "print('Se demoró {:.2f} minutos.'.format(duración/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76603d",
   "metadata": {},
   "source": [
    "El resultado correspondería a la configuración de hiperparámetros del mejor modelo posible, usando árbol de decisión (dentro del espacio de hiperparámetros que probamos). La teoría es que si llegasen nuevos casos, este sería el árbol que mejor los clasifica (de los que hemos probado). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9216691",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1234, \n",
    "                             max_depth= grid_search.best_params_['max_depth'],\n",
    "                             criterion= grid_search.best_params_['criterion'],\n",
    "                             min_samples_split= grid_search.best_params_['min_samples_split'],\n",
    "                             min_samples_leaf= grid_search.best_params_['min_samples_leaf'],\n",
    "                             ccp_alpha=grid_search.best_params_['ccp_alpha'])\n",
    "model = clf.fit(X_train, y_train)\n",
    "plt.figure(figsize=(400,100))\n",
    "tree.plot_tree(clf, feature_names=[\"clase\", \"género\", \"edad\", \n",
    "                                   \"conyugues-hermanos\", \"padres-hijos\" , \"tarifa\"], \n",
    "               class_names=[\"No sobrevice\", \"Sí sobrevive\"], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01feecd7",
   "metadata": {},
   "source": [
    "Todavía podríamos considerar mejorar el árbol anterior, pero supongamos que quisieramos poder reportar la calidad esperada de este modelo, ¿con qué datos debieramos evaluarlo?... Correcto, acá corresponde usar el set de testeo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "print('MATRIZ DE CONFUSIÓN:')\n",
    "print(matriz_conf,'\\n')\n",
    "print('BIEN CLASIFICADOS: ', matriz_conf[0][0]+matriz_conf[1][1])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 0: ', matriz_conf[0][0])\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 1: ', matriz_conf[1][1])\n",
    "print('MAL CLASIFICADOS: ', matriz_conf[0][1]+matriz_conf[1][0])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 1: ', matriz_conf[0][1],'(falso positivo)')\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 0: ', matriz_conf[1][0],'(falso negativo)')\n",
    "print('La exactitud de testeo del modelo es: {:.3f}'.format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ed4a4",
   "metadata": {},
   "source": [
    "Observe que esta exactitud que reportamos está limpia de sobreajuste. Ello es porque los datos de testeo con que se reporta el resultado final no fueron conocidos por el modelo ni durante entrenamiento ni a través de la búsqueda exaustiva de hiperparámetros como validación. Ello lo podemos lograr aplicado uno de dos métodos para organizar nuestros experimentos: \n",
    "\n",
    "<ul>\n",
    "    <li>Trabajar con tres set de datos: entrenamiento, validación y testeo. (típicamente 60 - 20 - 20 %)</li>\n",
    "    <li>Trabajar con dos set de datos: entrenamiento y testeo (80 - 20 %), usando validación cruzada para validación.</li>\n",
    "</ul>\n",
    "\n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:828/format:webp/1*4G__SV580CxFj78o9yUXuQ.png width=\"400\"/>\n",
    "\n",
    "En ambos casos la actividad de validación consiste en validar la selección de modelos y hiperparámetros con que se entrenen. En ambos casos la muestra de testeo no se debe usar mientras no se llegue a la etapa final de prueba de resultados, evitando así cualquier riesgo de sobreajuste sobre los datos con que se evaluará. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30741da4",
   "metadata": {},
   "source": [
    "<b>Importate</b>: Si nos preguntan cuál es la exactitud esperada del modelo, cuando enfrente nuevos casos, debemos reportar la exactitud de testeo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f9538",
   "metadata": {},
   "source": [
    "Hásta acá hemos visto dos modelos para clasificación: Regresión Logística y Árboles de Decisión. ¿Qué otros modelos de clasificación podríamos probar? En lo que sigue de este apunte recorreremos algunas otras opciones disponibles.\n",
    "\n",
    "Pero antes...\n",
    "\n",
    "<b>NORMALIZACIÓN</b>\n",
    "\n",
    "Una omisión que ya debieramos resolver, es que hasta ahora estamos operando con los datos del Titanic sin normalizar. Esto pudo no ser tan relevantes cuando nuestros modelos eran de tipo Árbol de Decisión, pero los modelos basados en distancia como KNN y SVM (que se presentan a continuación) sí que se verían beneficiados si normalizaramos. Esto ocurre porque al estar en distinta escala cada atributo, el aporte de un atributo a la distancia entre dos observaciones va a depender de las magnitudes de sus valores. Variables que están en el orden de los millones tenderían a tapar la diferencia de variables en las unidades, aunque ocurra que la distancia relativa es mayor en estas últimas.  \n",
    "\n",
    "Para los próximos experimentos aplicaremos Normalización Min-Max:\n",
    "\n",
    "$$ x' = \\frac{{x - \\min(x)}}{{\\max(x) - \\min(x)}} $$\n",
    "\n",
    "Si bien está disponible en librerías, por ahora lo implementaremos nosotros, a fin de asegurar que nos queda claro qué estamos haciendo.\n",
    "\n",
    "Ahora sí, pasemos a estudiar otros modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4580530",
   "metadata": {},
   "source": [
    "<b>VECINOS MÁS CERCANOS - $K$-NEAREST NEIGHBORS</b>\n",
    "\n",
    "El algoritmo de clasificación de vecinos más cercanos <i>$K$-Nearest Neighbors (KNN)</i> es un método simple y ampliamente utilizado. A diferencia de otros algoritmos, como los basados en Árboles de decisión, KNN no realiza un entrenamiento explícito. En lugar de eso, simplemente registra cada observación de los datos de entrenamiento, con su clase correspondiente, y cuando se le pide hacer una predicción para una nueva observación busca los $k$ vecinos más cercanos a dicha nueva observación, revisa la clase de esos $k$ vecinos y selecciona la moda como clase de la nueva observación. Tiene un parecido con el algoritmo <i>K-means</i> que vimos al estudiar clustering, aunque aplicado a clasificación. \n",
    "\n",
    "<br> <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png width=\"200\"/>\n",
    "\n",
    "Probemos este modelo en el problema del Titanic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Uso del pipeline\n",
    "X, y = preprocesar_titanic(\"Titanic.csv\")\n",
    "\n",
    "# Acá agregamos la Normalización. \n",
    "# en este caso, usaremos Normalización Min-Max\n",
    "# La hemos implementado a mano en lugar de importarla de librería\n",
    "# pero en el futuro sólo la importaremos ya implementada, \n",
    "# y la agregaremos al pipeline de preprocesmiento.\n",
    "for feature in range(len(X[0])):\n",
    "    min = None\n",
    "    max = None\n",
    "    for row in range(len(X)):\n",
    "        if min == None or X[row,feature] < min:\n",
    "            min = X[row,feature]\n",
    "        if max == None or X[row,feature] > max:\n",
    "            max = X[row,feature]\n",
    "    for row in range(len(X)):\n",
    "        X[row,feature] = (X[row,feature] - min)/(max - min)\n",
    "\n",
    "# Separamos la muestra al azar, 80% para entrenar y 20% para testeo final. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c36f14",
   "metadata": {},
   "source": [
    "Usaremos la librería sklearn.neighbors, de la que importaremos KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Definimos un modelo de tipo vecinos cercanos\n",
    "model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc44534",
   "metadata": {},
   "source": [
    "KNN es un modelo muy simple de explicar, y además requiere casi nada de esfuerzo de entrenar. Básicamente sólo es necesario cargar los datos de entrenamiento para que estén disponibles para comparación, no realiza ningún cálculo adicional. \n",
    "\n",
    "El costo de realizar predicciones, sin embargo, es bastante mayor que con otros modelos. Cada vez que se requiere una predicción sobre una nueva observación es necesario computar la distancia de la nueva observación con cada una de las observaciones disponibles en la muestra de entrenamiento, para identificar las $k$ más cercanas. Esto se vuelve pesado con muestras grandes, ya sea en cantidad de observaciones, pero también en cantidad de dimensiones de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos nuestro clasificador\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "acc = (matriz_conf[0][0]+matriz_conf[1][1])/(matriz_conf[0][0]+matriz_conf[1][1]+\n",
    "                                            matriz_conf[0][1]+matriz_conf[1][0])\n",
    "print('MATRIZ DE CONFUSIÓN para modelo KNN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('BIEN CLASIFICADOS: ', matriz_conf[0][0]+matriz_conf[1][1])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 0: ', matriz_conf[0][0])\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 1: ', matriz_conf[1][1])\n",
    "print('MAL CLASIFICADOS: ', matriz_conf[0][1]+matriz_conf[1][0])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 1: ', matriz_conf[0][1],'(falso positivo)')\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 0: ', matriz_conf[1][0],'(falso negativo)')\n",
    "print('La exactitud de testeo del modelo KNN es: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f43e6d",
   "metadata": {},
   "source": [
    "<b>$k$ vecinos</b>\n",
    "\n",
    "¿Cuántos vecinos está tomando en cuenta el modelo que hemos entrenado? sólo nos hemos referido a esta cantidad como un número $k$, pero en ningún momento hemos definido que cantidad es esta, en este caso. Ocurre que es un hiperparámetro del modelo, y en la implementación KNeighborsClassifier de sklearn.neighbors el hiperparámetro tiene por defecto valor $k=5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "best_acc_seen = 0\n",
    "best_k_seen = 0\n",
    "print('k  ---  acc')\n",
    "for k in range(1,100):\n",
    "    # Definimos un modelo de tipo vecinos cercanos\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=5)\n",
    "    acc = cv_results['test_score'].mean()\n",
    "    print(k,'---',acc)\n",
    "    if acc > best_acc_seen:\n",
    "        best_acc_seen = acc\n",
    "        best_k_seen = k\n",
    "print('El mejor k fue {} vecinos, con accuracy de: {:.3f}'.format(best_k_seen,best_acc_seen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f6eb9",
   "metadata": {},
   "source": [
    "<b>Distancia</b>\n",
    "\n",
    "Cada vez que decimos haber seleccionado a los $k$ vecinos más cercanos, ¿de qué estamos hablando exactamente? CORRECTO, de las mismas métricas de distancia de las que hablamos cuando estudiamos clustering (agrupamiento)\n",
    "\n",
    "KNeighborsClassifier maneja un hiperparámetro para identificar la métrica de distancia, llamado <i>metric</i>, el cual por defecto tiene como valor la distancia de <b><i>Minkowski</i></b>. \n",
    "\n",
    "$$d_{p}(a,b)=\\left(\\sum_{i=1}^{n} \\left| a_i - b_i \\right|^p \\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "donde $p$ es un número real mayor que 1. Ya habíamos visto que si $p=2$ se forma la conocida Distancia Euclidiana, que si $p=1$ se denomina Distancia Manhattan, mientras que si $p \\to\\infty$ pasa a ser la Distancia de Chebyshev. \n",
    "\n",
    "<br> <img src=https://iq.opengenus.org/content/images/2018/12/distance.jpg\n",
    "          width=\"400\"/>\n",
    "         \n",
    "KNeighborsClassifier maneja un hiperparámetro <b><i>p</i></b> para definir la <b>potencia</b> aplicada a la métrica distancia de Minkowski, siendo su valor por defecto $p=2$. Esto significa que por defecto aplica la Distancia Euclidiana. Si cambiamos en valor a $p=1$ se transforma en Distancia Manhattan. Si incrementamos el valor de $p$ por encima de dos, iremos progresivamente acercándonos a la Distancia de Chebyshev, lo que implica que cada vez le da más importancia a la dimensión que aporte más diferencia entre dos observaciones, cuando mide la distancia entre ellas. \n",
    "\n",
    "¿Cuál de estas métricas debiéramos usar? o dicho de otra forma, ¿qué valor debiéramos configurar para $p$? Como ya estamos acostumbrados, no es una pregunta que debamos responder nosotros, cuando podemos dejar que los datos hablen por si mismo, a través de una búsqueda exhaustiva en que comparemos el desempeño de distintas configuraciones. \n",
    "\n",
    "Busquemos la cantida $k$ de vecinos y la potencia $p$ de distancia que mejor clasifiquen la supervivencia de nuestros pasajeros del Titanic, usando <i>Grid Search</i>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definimos un modelo de tipo vecinos cercanos\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Definir la grilla de hiperparámetros a probar\n",
    "param_grid = {'n_neighbors': range(1,30),\n",
    "              'p': [a/10 for a in range(10,30,5)],\n",
    "             }\n",
    "\n",
    "# Realizamos búsqueda de grilla con validación cruzada de 5 carpetas\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "\n",
    "# Ajustar el modelo con búsqueda de grilla\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimimos mejores hiperparámetros y mejor puntaje\n",
    "print(\"Mejores hiperparámetros: \", grid_search.best_params_)\n",
    "print(\"Mejor puntaje: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b44fd5",
   "metadata": {},
   "source": [
    "<b>Vecinos Cercanos Ponderados - <i>Weighted K-NN</i></b>\n",
    "\n",
    "KNN selecciona la moda de las clases de los que clasifican como vecinos cercanos, sin discriminar aquellos vecinos que están muy cercanos respecto a los que quedan algo más lejos. Podemos intentar incrementar la exactitud de la clasificación dándole ponderación al voto de cada vecino, de acuerdo con la distancia a la que se encuentre. Esto significa que pesará más el voto del vecino que califique como el más cercano de todos, y cada uno de los siguientes vecinos, hasta el número $k$ más lejano, tendrán un voto progresivamente menor. \n",
    "\n",
    "<br> <img src=https://www.jeremyjordan.me/content/images/2017/06/Screen-Shot-2017-06-18-at-3.15.19-PM.png\n",
    "          width=\"500\"/>\n",
    "\n",
    "\n",
    "KNeighborsClassifier maneja un hiperparámetro <b><i>weights</i></b> que por defecto tiene valor <i>uniform</i> pero se puede cambiar a <i>distance</i> para que utilice el inverso de la distancia como ponderador. \n",
    "\n",
    "Nuevamente, no sabemos si ponderar la votación incremente o perjudique la clasificación, dependerá de cada problema y el comportamiento de sus datos si conviene o no. Así que podemos agregar este hiperparámetro a nuestra búsqueda en grilla, y ver en validación cruzada cual configuración resulta mejor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150614f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un modelo de tipo vecinos cercanos\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Definir la grilla de hiperparámetros a probar\n",
    "param_grid = {'n_neighbors': range(1,30),\n",
    "              'p': [a/10 for a in range(10,30,5)],\n",
    "              'weights': ['distance', 'uniform'],\n",
    "             }\n",
    "# Realizamos búsqueda de grilla con validación cruzada de 5 carpetas\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "# Ajustar el modelo con búsqueda de grilla\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Imprimimos mejores hiperparámetros y mejor puntaje\n",
    "print(\"Mejores hiperparámetros: \", grid_search.best_params_)\n",
    "print(\"Mejor puntaje: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead6dc7",
   "metadata": {},
   "source": [
    "<b>Testeo de resultados</b>\n",
    "\n",
    "Volvamos a reportar la exactitud de nuestro modelo KNN, usando la configuración óptima de acuerdo con el anterior análisis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7340235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un modelo de tipo vecinos cercanos con hiperparámetros óptimos\n",
    "model = KNeighborsClassifier(n_neighbors=grid_search.best_params_['n_neighbors'],\n",
    "                          p=grid_search.best_params_['p'],\n",
    "                          weights=grid_search.best_params_['weights'])\n",
    "\n",
    "\n",
    "# Entrenamos nuestro clasificador\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "acc = (matriz_conf[0][0]+matriz_conf[1][1])/(matriz_conf[0][0]+matriz_conf[1][1]+\n",
    "                                            matriz_conf[0][1]+matriz_conf[1][0])\n",
    "print('MATRIZ DE CONFUSIÓN para modelo KNN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('BIEN CLASIFICADOS: ', matriz_conf[0][0]+matriz_conf[1][1])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 0: ', matriz_conf[0][0])\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 1: ', matriz_conf[1][1])\n",
    "print('MAL CLASIFICADOS: ', matriz_conf[0][1]+matriz_conf[1][0])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 1: ', matriz_conf[0][1],'(falso positivo)')\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 0: ', matriz_conf[1][0],'(falso negativo)')\n",
    "print('La exactitud de testeo del modelo KNN es: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e54637",
   "metadata": {},
   "source": [
    "Este es el accuracy que esperaríamos lograr clasificando a nuevos pasajeros del Titanic con nuestro KNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992be4da",
   "metadata": {},
   "source": [
    "<b>MÁQUINA DE VECTOR SOPORTE - <i>SUPPORT VECTOR MACHINE</i></b>\n",
    "\n",
    "Al inicio de la unidad de aprendizaje supervisado, con el problema de clasificación, lo primero que vimos fue el enfoque estadístico basado en un clasificador lineal como es Regresión Logística. Encontramos que la limitación de ese enfoque ocurre cuando los datos no son linealmente separables, lo que nos llevó a buscar otros enfoques no lineales. Lo malo es que, con eso, nos encontramos con modelos que tienden a ser más complejos y propensos al sobreajuste. \n",
    "\n",
    "Volvamos al enfoque de generar un hiperplano que separe las clases, para ello se propone un modelo basado en optimización por la vía de identificar <i>vectores de soporte</i>,  observaciones que se encuentren más cerca del hiperplano de separación entre las clases. El hiperplano separador se ajusta buscando maximizar el margen entre estos vectores de soporte. \n",
    "\n",
    "Para implementar Support Vector Machine en Python recurriremos, una vez más, a sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creaamos un clasificador SVM\n",
    "clf = svm.SVC(kernel='linear')  # Kernel lineal\n",
    "\n",
    "# Lo entrenamos\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos predicciones en el conjunto de testeo y reportamos\n",
    "y_hat = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_hat)\n",
    "print(\"Exactitud del clasificador SVM: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a838e",
   "metadata": {},
   "source": [
    "Podría parecer que SVM es similar a Regresión Logística, en el sentido de que su clasificación se basa en un hiperplano y por tanto sería un clasificador lineal, con las mismas limitaciones que regresión logística. Sin embargo, SVM tiene una característica adicional, el uso de <b><i>kernels</i></b> que consisten en agregar dimensiones adicionales de forma que los datos sí se vuelvan separables. \n",
    "\n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:1400/1*mCwnu5kXot6buL7jeIafqQ.png\n",
    "          width=\"400\"/>\n",
    "          \n",
    "Los kernels son una medida de distancia en un espacio dimensional transformado, el truco está en que no necesitamos llegar a la transformación para medir la distancia entre dos observaciones. \n",
    "\n",
    "Hay distintos kernels, algunos de ellos son:\n",
    "\n",
    "<b>Kernel Lineal</b>: Es el kernel más simple y directo. No realiza ninguna transformación en los datos de entrada. Simplemente realiza el producto escalar entre los vectores de características originales. El kernel lineal se define como:\n",
    "\n",
    "$$K_{lineal}(a, b) = a \\cdot b$$\n",
    "\n",
    "Donde a y b son observaciones, es decir vectores con valores de cada atributo (variable predictora). Este kernel es efectivo cuando los datos son linealmente separables en el espacio original, puesto que en realidad no tienen el efecto de incrementar la dimensionalidad del espacio transformado... Es un tema algo complejo en que no vamos a profundizar mucho, pero digamos que puede llegar a resultados equivalente a la Regresión Logística. \n",
    "\n",
    "<b>Kernel Polinómico</b>: Mapea los datos de entrada a un espacio de características de mayor dimensión utilizando una función polinómica. El kernel polinómico se define como:\n",
    "\n",
    "$$K_{polinómico}(a, b) = (\\gamma * (a \\cdot b) + \\text{coef0})^{\\text{degree}}$$\n",
    "\n",
    "Donde gamma, coef0 y degree son hiperparámetros ajustables. El hiperparámetro degree controla el grado del polinomio y gamma ajusta la influencia de los términos de interacción. El coef0 permite controlar el término independiente. El kernel polinómico es útil cuando los datos tienen una estructura polinómica y no son linealmente separables en su dimensionalidad original. Entre mayor el grado más dimensiones tendrá el espacio transformado, y con eso aumenta la posibilidad de que los datos sean linealmente separables.\n",
    "\n",
    "<b>Kernel de Base Radial (RBF)</b>: También conocido como Gaussian Kernel, utiliza una función de base radial para mapear los datos a un espacio de características de dimensión infinita. El kernel RBF se define como:\n",
    "\n",
    "$$K_{RBF}(a, b) = exp(-\\gamma * ||a - b||^2)$$\n",
    "\n",
    "Donde gamma es un hiperparámetro ajustable que controla la influencia del término exponencial. El kernel RBF es efectivo para datos no linealmente separables y es capaz de capturar patrones complejos en los datos.\n",
    "\n",
    "sklearn.svm.SVC usa RBF como kernel por defecto, y su hiperparámetro gamma está definido por la operación scale: \n",
    "\n",
    "$$ \\gamma = \\frac{1}{(n_{features} * \\sigma^{2}(X))}$$\n",
    "\n",
    "La escala consiste en dividir por el número de atributos multiplicado por la varianza de los datos sobre los mismos atributos. \n",
    "\n",
    "Ya hemos explorado SVM con kernel lineal. Probemos el kernel polinomial a continuación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa878e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Creaamos un clasificador SVM\n",
    "clf = svm.SVC(kernel='poly')  # Kernel polinomial, grado por defecto =3\n",
    "\n",
    "# Lo entrenamos\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos predicciones en el conjunto de testeo y reportamos\n",
    "y_hat = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_hat)\n",
    "print(\"Exactitud del clasificador SVM: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6a99b",
   "metadata": {},
   "source": [
    "Vemos si podemos mejorar este resultado eligiendo el hiperparámetro coef0, que por defecto es 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37807e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definimos un modelo de SVM\n",
    "model = svm.SVC(kernel='poly')\n",
    "\n",
    "# Definir la grilla de hiperparámetros a probar\n",
    "param_grid = {'coef0': [-10,-1,0,1,10,100,1000,10000],\n",
    "             }\n",
    "\n",
    "# Realizamos búsqueda de grilla con validación cruzada de 5 carpetas\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# Ajustar el modelo con búsqueda de grilla\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimimos mejores hiperparámetros y mejor puntaje\n",
    "print(\"Mejores hiperparámetros: \", grid_search.best_params_)\n",
    "print(\"Mejor puntaje: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d79c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la grilla de hiperparámetros a probar\n",
    "param_grid = {'coef0': [750,1000,1250,1500,1750,2000],\n",
    "             }\n",
    "\n",
    "# Realizamos búsqueda de grilla con validación cruzada de 5 carpetas\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# Ajustar el modelo con búsqueda de grilla\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimimos mejores hiperparámetros y mejor puntaje\n",
    "print(\"Mejores hiperparámetros: \", grid_search.best_params_)\n",
    "print(\"Mejor puntaje: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f46c86",
   "metadata": {},
   "source": [
    "Fíjese que hemos realizado dos grid search para acercarnos más a un eventual Coef0 óptimo. En el primero usamos un incremento exponencial porque no tenemos mucha referencia previa de donde podría esta nuestro óptimo, mientras que la segunda búsqueda fue lineal, alrededor de donde teníamos mejor resultado. \n",
    "\n",
    "Todavía podemos incrementar el grado de nuestro polinomio. Hagamos pruebas con variaciones de Coef0 y grado, los valores en la grilla los seleccionamos tratando de cubrir un rango amplio y explorando con mayor detalle las zonas donde creemos que hay mejores resultados, pero también considerando el tiempo de búsqueda de la exploración. Incrementar el grado del kernel polinomial genera un aumento de la complejidad del modelo, que rápidamente aumenta el tiempo de ejecución, así que lo mantenemos acotado. \n",
    "\n",
    "Todavía podemos probar otro kernel. Incluyamos en la búsqueda el kernel entre las opciones de la grilla, agreguemos RBF que nos falta por probar.\n",
    "\n",
    "Vamos a comenzar a reportar el tiempo de ejecución de la búsqueda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Esto se va a demorar, registremos cuanto. \n",
    "inicio = time.time()\n",
    "\n",
    "# Definir la grilla de hiperparámetros a probar\n",
    "# La grilla está muy limitada para reducir tiempos de ejecución\n",
    "# la idea es buscar con tiempo una colección más grande de combinaciones. \n",
    "param_grid = {'coef0': [75,100,125],\n",
    "              'degree': [2,3,4],\n",
    "              'kernel': ['linear','poly','rbf']\n",
    "             }\n",
    "# Definimos un modelo de SVM\n",
    "model = svm.SVC()\n",
    "\n",
    "# Realizamos búsqueda de grilla con validación cruzada de 5 carpetas\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# Ajustar el modelo con búsqueda de grilla\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "\n",
    "# Imprimimos mejores hiperparámetros y mejor puntaje\n",
    "print(\"Mejores hiperparámetros: \", grid_search.best_params_)\n",
    "print(\"Mejor puntaje: \", grid_search.best_score_)\n",
    "print('Se demoró {:.2f} minutos.'.format(duración/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c634a",
   "metadata": {},
   "source": [
    "<b>Testeo de resultados</b>\n",
    "\n",
    "Volvamos a reportar la exactitud de nuestro modelo SVM, usando la configuración óptima de acuerdo al análisis anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca204a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un modelo de tipo SVM con hiperparámetros óptimos\n",
    "model = svm.SVC(coef0=grid_search.best_params_['coef0'],\n",
    "              degree=grid_search.best_params_['degree'],\n",
    "              kernel=grid_search.best_params_['kernel'])\n",
    "\n",
    "# Entrenamos nuestro clasificador\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "acc = (matriz_conf[0][0]+matriz_conf[1][1])/(matriz_conf[0][0]+matriz_conf[1][1]+\n",
    "                                            matriz_conf[0][1]+matriz_conf[1][0])\n",
    "print('MATRIZ DE CONFUSIÓN para modelo SVM')\n",
    "print(matriz_conf,'\\n')\n",
    "print('BIEN CLASIFICADOS: ', matriz_conf[0][0]+matriz_conf[1][1])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 0: ', matriz_conf[0][0])\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 1: ', matriz_conf[1][1])\n",
    "print('MAL CLASIFICADOS: ', matriz_conf[0][1]+matriz_conf[1][0])\n",
    "print('\\tPasajeros con sobrevivencia = 0 clasificados como 1: ', matriz_conf[0][1],'(falso positivo)')\n",
    "print('\\tPasajeros con sobrevivencia = 1 clasificados como 0: ', matriz_conf[1][0],'(falso negativo)')\n",
    "print('La exactitud de testeo del modelo SVM es: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c2931",
   "metadata": {},
   "source": [
    "<b>REDES NEURONALES ARTIFICIALES - <i>ARTIFICIAL NEURAL NETWORK</i></b>\n",
    "\n",
    "Probaremos a continuación un modelo basado en un tipo de topología completamente diferente, que se inspira en el funcionamiento del cerebro humano y están diseñadas para el procesamiento de información y el aprendizaje automático en máquinas, las ya famosas Redes Neuronales (artificiales). Estos modelos (al igual que los anteriores) se pueden usar tanto en regresión como en clasificación, en este apunte sólo vemos su versión para clasificación.   \n",
    "\n",
    "Una red neuronal está formada por neuronas conectadas entre sí, organizadas en capas. Cada neurona es una célula con la capacidad de activarse frente a un estímulo. La magnitud de la señal de entrada, que es su estímulo, determina si la neurona se activa o no. Cuando una neurona se activa emite una señal que, a su vez, podría o no estimular a otras neuronas conectadas a ella. \n",
    "\n",
    "El estímulo inicial es, por ejemplo, luz que estimula una primera capa de células, las que transmiten el estímulo a una segunda capa y así. Cada capa puede especializarse en distintos aspectos, por ejemplo, una capa es sensible a algunos rangos de luz y no a otros. Al final de la red saldrá una señal que no es la luz que activó el sistema, pero puede representar la información relevante que esa señal de luz contenía, en relación con algún propósito o problema en particular. \n",
    "<br> <img src=https://www.sciencenews.org/wp-content/uploads/2015/05/opto_opener.png\n",
    "          width=\"400\"/>\n",
    "          \n",
    "Una <b>ANN</b> es un modelo matemático que se basa en la misma mecánica, pero en lugar de estar implementado sobre células biológicas, se compone de células virtuales instanciadas en un computador. \n",
    "<br> <img src=https://www.researchgate.net/profile/Syed_Saqib_Jamal/publication/351443166/figure/fig2/AS:1021624551743489@1620585682403/A-biological-neuron-in-comparison-to-an-artificial-neural-network-a-Brain-neuron-b.png\n",
    "          width=\"400\"/>\n",
    "          \n",
    "¿En qué consiste una célula de una ANN? La unidad básica de este tipo de modelos es a su vez un modelo matemático relativamente simple, llamado <b><i>Perceptron</i></b>. La señal de entrada se compone de una o muchas variables independientes, que corresponden a las variables predictoras, además de un término constante que corresponde a un intercepto. El valor de cada una de estas variables se multiplica por un peso, parámetro usualmente identificado con la letra $w$, para luego ser sumados formando una función lineal. El valor que se determine como resultado de la función lineal es recibido por una función de activación, la cual devolverá como respuesta 0 si su entrada es menor a un cierto umbral, y 1 si la entrada es mayor a dicho umbral.  \n",
    "<br> <img src=https://blog.paperspace.com/content/images/2020/10/perceptron-6168423.jpg\n",
    "          width=\"400\"/>\n",
    "\n",
    "¿Le suena familiar esta descripción? ya hemos visto un modelo que se comporta aproximadamente así... Regresión Logística. Así que no nos sorprenderá descubrir que con un perceptron sólo podemos separar observaciones que sean linealmente separables, y no puede separar clases que no sean linealmente separable. Podría pensarse en un truco del tipo kernel para darle esta capacidad, pero ahí es donde entra en juego una estrategia diferente, que ha resultado ser altamente exitosa por su versatilidad y eficiencia: ¿para qué usar un perceptrón si podemos usar muchos perceptrones conectados? Los cerebros de los animales (incluidos nosotros) se aprovechan de este tipo de estructura y logran resolver problemas complejos. \n",
    "\n",
    "Así nace el siguiente modelo, que ya es una ANN, que se denomina <b><i>Multi-Layer Perceptron</i></b>. Consiste en instanciar varios perceptrones que reciben las variables de entrada, cada uno con sus propios pesos asociados a dichas variables (más intercepto), formando así una <b>capa</b> de perceptrones. Cada uno de estos perceptrones es un <b>nodo</b> de la red neuronal. La salida de las señales que emita esta primera capa de perceptrones se utiliza como entrada de una siguiente capa de perceptrones, pudiendo acumularse varias capas de esta forma. \n",
    "\n",
    "El número de perceptrones en cada capa es independiente, no necesitan ser del mismo tamaño, muchas veces vamos a querer apilar capas de distinto tamaño. La última capa en esta secuencia debe tener sólo las salidas que haga falta al problema en cuestión, si es un problema de clasificación binario basta con una salida que pueda ser cero o uno, por ejemplo, pero cuando son más de dos clases resulta recomendable que exista un nodo por cada clase de forma que cada salida esté explícitamente representada. \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:563/1*4_BDTvgB6WoYVXyxO8lDGA.png\n",
    "          width=\"350\"/>\n",
    "          \n",
    "Entonces, cuando construimos una ANN para clasificación primero organizamos las variables independientes para alimentar una capa de entrada cuyo tamaño corresponderá a las dimensiones del problema. A continuación, agregamos al menos una o bien varias capas intermedias, que llamaremos capas ocultas. Finalmente habrá una capa de salida cuyo tamaño debe corresponder a la cantidad de clases que se requiera identificar.\n",
    "\n",
    "Podríamos programar esta topología directamente, pero como ya está hecho y disponible en librerías, vamos a aprovechar el módulo Keras que provee interfaz a modelos de redes neuronales que están a su vez disponibles en la librería TensorFlow. Se trata de una de las implementaciones más populares de ANN disponibles en Pyhon, que nos permite obtener resultados con muy bajo esfuerzo en codificación.\n",
    "\n",
    "En caso de que no esté instalado Keras y/o TensorFlow, es necesario instalarlos usando PIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por si hemos perdido los datos, los volveremos a cargar.\n",
    "# si estamos ejecutando de una sola vez el apunte, esto no es necesario\n",
    "# pero como muchas veces vamos a estar ejecutando por partes, mejor redundar.\n",
    "\n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Uso del pipeline\n",
    "X, y = preprocesar_titanic(\"Titanic.csv\")\n",
    "\n",
    "for feature in range(len(X[0])):\n",
    "    min = None\n",
    "    max = None\n",
    "    for row in range(len(X)):\n",
    "        if min == None or X[row,feature] < min:\n",
    "            min = X[row,feature]\n",
    "        if max == None or X[row,feature] > max:\n",
    "            max = X[row,feature]\n",
    "    for row in range(len(X)):\n",
    "        X[row,feature] = (X[row,feature] - min)/(max - min)\n",
    "\n",
    "# Separamos la muestra al azar, 80% para entrenar y 20% para testeo final. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede55d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "\n",
    "# Creamos un modelo de la red neuronal declarando una secuencia a la que agregaremos capas. \n",
    "model = Sequential()\n",
    "\n",
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende de la cantidad de Xs.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Agregamos la primera capa oculta, donde además declaramos la dimensión de la capa de entrada.\n",
    "# Debemos establecer cuantos nodos tiene esta capa oculta.\n",
    "# Debemos declarar la función de activación de estos nodos. \n",
    "model.add(Dense(units=8, activation='relu', input_dim=entrada_dim))\n",
    "\n",
    "# Agregamos una capa de salida con sólo un nodo (0 o 1)\n",
    "# Declaramos la función de activación del nodo de salida. \n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilamos el modelo indicando la función de perdida, optimizador y métricas.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Registremos cuanto se demora. \n",
    "inicio = time.time()\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento\n",
    "# Haremos 10 iteraciones de entrenamiento\n",
    "# en cada una de ellas los datos de entrenamiento se usarán de uno en uno.\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "\n",
    "# Evaluamos el modelo con datos de testeo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Se demoró {:.2f} minutos.'.format(duración/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df1393",
   "metadata": {},
   "source": [
    "<b>Controlando la aleatoriedad</b>\n",
    "\n",
    "Si repetimos el experimento vamos a observar que los resultados cambian. Esto se debe a que el algoritmo que ajusta los pesos de la ANN durante el entrenamiento utiliza aleatoriedad, por ejemplo al establecer un punto de partida. Dado que estamos usando una arquitectura más compleja, en la que intervienen varias librerías, no podemos fijar esta aleatoriedad con un simple random_state o seed como hemos hecho en otras ocasiones. La solución que se presenta a continuación asegura resultados reproducibles para un computador, pero no necesariamente dará el mismo resultado en otros equipos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Para lograr resultados estables en un equipo,\n",
    "# vamos a tener que definir seed en distintas intancias. \n",
    "seed_value= 1234\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Creamos un modelo de la red neuronal declarando una secuencia a la que agregaremos capas. \n",
    "model = Sequential()\n",
    "\n",
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende de la cantidad de Xs.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Agregamos la primera capa oculta, donde además declaramos la dimensión de la capa de entrada.\n",
    "# Debemos establecer cuantos nodos tiene esta capa oculta.\n",
    "# Debemos declarar la función de activación de estos nodos. \n",
    "model.add(Dense(units=8, activation='relu', input_dim=entrada_dim))\n",
    "\n",
    "# Agregamos una capa de salida con sólo un nodo (0 o 1)\n",
    "# Declaramos la función de activación del nodo de salida. \n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilamos el modelo indicando la función de perdida, optimizador y métricas.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Registremos cuanto se demora. \n",
    "inicio = time.time()\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento\n",
    "# Haremos 10 iteraciones de entrenamiento\n",
    "# en cada una de ellas los datos de entrenamiento se usarán de uno en uno.\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "fin = time.time() \n",
    "duración = fin - inicio\n",
    "\n",
    "# Evaluamos el modelo con datos de testeo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Se demoró {:.2f} minutos.'.format(duración/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21dd538",
   "metadata": {},
   "source": [
    "<b>Hiperparámetros de la ANN</b>\n",
    "\n",
    "Ahora que hemos fijado la aleatoriedad, podemos comenzar a ajustar hiperparámetros de la ANN. ¿Qué hiperparámetros tenemos a nuestra disposición? Por ahora sólo recorreremos algunos. \n",
    "\n",
    "El hiperparámetro <b><i>units</i></b> del método <b><i>add</i></b> cuando se agregan capas al modelo, se refiere a la cantidad de nodos que tendrá esa capa. <b><i>activation</i></b> en el mismo método es para establecer la función de activación. En otro momento veremos alternativas de función, <b><i>relu</i></b> es una de las alternativas más usadas para capas ocultas, mientras que <b><i>sigmoid</i></b> es más frecuente en capas de salida, aunque también se solía usar en capas ocultas.\n",
    "\n",
    "Al compilar el modelo con el método <b><i>compile</i></b> podemos elegir la función de pérdida, o <b><i>loss</i></b>, donde <b><i>binary_crossentropy</i></b> es una alternativa típica para problemas de clasificación. También corresponde definir el algoritmo de optimización, con el hiperparámetro <b><i>optimizer</i></b>, donde estamos dejando <b><i>adam</i></b>. Estos hiperparámetros son muy relevantes respecto a como aprenderá el modelo, dejaremos para más adelante estudiar su significado y rol durante el proceso de aprendizaje.  \n",
    "\n",
    "Durante el entrenamiento, al usar el método <b><i>fit</i></b>, el hiperparámetro <b><i>epochs</i></b> se refiere a cuantas iteraciones o pasadas sobre los datos se hará. La ANN puede seguir aprendiendo cuando se entrena nuevamente sobre los mismos datos, algo que en general no ocurre con otros modelos. Así que podemos volver a aprovechar los datos de entrenamiento tanto como nos parezca prudente. Pero, entre más epochs usemos más se va a demorar, afortunadamente el tiempo crece linealmente (no exponencial como pasa en otros casos). \n",
    "\n",
    "Mientras el modelo se entrena puede consumir los datos de uno en uno o en <b><i>baths</i></b> o lotes de un cierto tamaño controlado por el hiperparámetro <b><i>bath_size</i></b>. Entre más grande sea el tamaño de lote más rápido será el entrenamiento, pero por otro lado el ajuste de los pesos se hará con menos frecuencia y puede hacer más lento el aprendizaje de cada epoch. \n",
    "\n",
    "No afecta el aprendizaje, pero durante el entrenamiento también podemos usar el hiperparámetro <i>verbose</i></b> para controlar cuanta información queremos que salga en pantalla durante cada epoch. Por ejemplo, en los próximos experimentos incrementaremos las epochs y queremos resumir la información en una gráfica, así que ya no nos conviene imprimir el avance en cada una de ellas.  \n",
    "\n",
    "<b>Grafica del aprendizaje de la red</b>\n",
    "\n",
    "Haremos lo siguiente, vamos a validar los resultados que va obteniendo el modelo después de entrenar cada una de 100 epochs, y graficaremos al final el accuracy obtenido con los datos de entrenamiento y también con los datos de testeo. Eso nos permitirá ir entendiendo como aprende progresivamente nuestra red neuronal, mientras la entrenamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed\n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=8, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Junto con entrenar, vamos a guardar el accuracy de entrenamient y validación,\n",
    "# en cada epoch, le tendremos que entregar el set de testeo para que haga validación\n",
    "# Aumentaremos los epochs de 10 a 100, es decir entrenaremos 100 veces con cada dato.\n",
    "# Vamos a usar batch_size = 32 para que procese en grupos de datos en lugar de 1 en 1, \n",
    "# y así se demore menos. \n",
    "# Además, vamos a decirle que no sea tan verboso, para que no muestre el avance de cada epoch.\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,  \n",
    "                    validation_data=(X_test, y_test),\n",
    "                   verbose = 0)\n",
    "\n",
    "# en history tenemos los valores de accuracy de entrenamiento y prueba, previamente guardados.\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# y necesitamos el rango de epochs para poder visualizar en la gráfica\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "# Graficamos el accuracy de entrenamiento y prueba en cada epoch\n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo final \n",
    "# con datos de entremamiento\n",
    "print('Train: ')\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "# y con datos de testeo\n",
    "print('Test: ')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d82103",
   "metadata": {},
   "source": [
    "Al analizar los resultados del accuracy durante el entrenamiento, observamos un patrón que se repetirá casi siempre: la exactitud del modelo va mejorando con el entrenamiento, y mejora más cuando se prueba con los mismos datos con que se entrenó respecto a si se prueba con datos de validación. En este caso, la exactitud de validación la estamos obteniendo a partir del set de datos que habíamos reservado para pruebas o testeo final... no perdamos de vista ese punto. \n",
    "\n",
    "¿Por qué la exactitud de entrenamiento es mejor que la de prueba? No nos debe extrañar que este modelo se ajuste muy bien a los datos con los que entrena, al punto que en realidad se sobre-ajusta. Ese margen adicional de exactitud que obtiene con los datos de entrenamiento, comparado con los datos de prueba, es señal del overfitting que le está afectando.\n",
    "\n",
    "<b>Incremento de la capacidad</b>\n",
    "\n",
    "¿Cómo podemos incrementar la exactitud del modelo? Hay varios hiperparámetros que podríamos optimizar, pero lo más relevante que podemos hacer es aumentar la topología de la red. Esto lo podemos hacer por dos vías, más nodos por capas o más capas en la secuencia. Probemos primero duplicar la cantidad de nodos en la capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# ¿Que tal si doblamos el número de nodos? \n",
    "model.add(Dense(units=16, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,  \n",
    "                    validation_data=(X_test, y_test),\n",
    "                   verbose = 0)\n",
    "\n",
    "# Gráfica de accuracy por epoch\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo final \n",
    "# con datos de entremamiento\n",
    "print('Train: ')\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "# y con datos de testeo\n",
    "print('Test: ')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be052a5c",
   "metadata": {},
   "source": [
    "Vemos que al aumentar los nodos el modelo mejora, tanto medido con los datos de entrenamiento como con los datos de prueba. La mejora se produce en mayor medida con los datos de entrenamiento, por razones que ya hemos comentado. \n",
    "\n",
    "¿Que pasaba si incrementábamos el tamaño del modelo por la vía de agregar capas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Dejamos los nodos que había originalmente,\n",
    "# y agregamos otra capa del mismo tamaño.\n",
    "model.add(Dense(units=8, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,  \n",
    "                    validation_data=(X_test, y_test),\n",
    "                   verbose = 0)\n",
    "\n",
    "# Gráfica de accuracy por epoch\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo final \n",
    "# con datos de entremamiento\n",
    "print('Train: ')\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "# y con datos de testeo\n",
    "print('Test: ')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb445ff",
   "metadata": {},
   "source": [
    "También se logra una mejora, y pareciera que se está manifestando en los datos de prueba como en entrenamiento. \n",
    "\n",
    "La buena noticia es que podemos incrementar la capacidad del sistema por las dos vías, más capas y más nodos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "# Agregemos más capas más y más nodos\n",
    "# podemos usar capas de distintos tamaños\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,  \n",
    "                    validation_data=(X_test, y_test),\n",
    "                   verbose = 0)\n",
    "\n",
    "# Gráfica de accuracy por epoch\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo final \n",
    "# con datos de entremamiento\n",
    "print('Train: ')\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "# y con datos de testeo\n",
    "print('Test: ')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc21fc",
   "metadata": {},
   "source": [
    "Observamos un incremento de la exactitud, tanto en entrenamiento como en prueba. Eso sí, la exactitud de prueba es menor que la de entrenamiento (observe que la línea roja está casi siempre bajo la línea azul).  \n",
    "\n",
    "<b>Incremento del Entrenamiento</b>\n",
    "\n",
    "¿Qué otra forma tenemos de sacarle más desempeño al modelo? podemos incrementar el entrenamiento aumentando en número de epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Aumentamos las epoch al doble\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32,  \n",
    "                    validation_data=(X_test, y_test),\n",
    "                   verbose = 0)\n",
    "\n",
    "# Gráfica de accuracy por epoch\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo final \n",
    "# con datos de entremamiento\n",
    "print('Train: ')\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "# y con datos de testeo\n",
    "print('Test: ')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab3df37",
   "metadata": {},
   "source": [
    "Por lo visto más entrenamiento es bueno... ¿y si aumentamos el entrenamiento mucho más?, seguramente podríamos mejorar todavía más los resultados del modelo y así nuestra ANN será más exacta..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed \n",
    "seed_value= 1234\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Aumentamos las epoch todavía más\n",
    "history = model.fit(X_train, y_train, epochs=800, batch_size=32,  \n",
    "                    validation_data=(X_test, y_test),\n",
    "                   verbose = 0)\n",
    "\n",
    "# Gráfica de accuracy por epoch\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo final \n",
    "# con datos de entremamiento\n",
    "print('Train: ')\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "# y con datos de testeo\n",
    "print('Test: ')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb31924",
   "metadata": {},
   "source": [
    "¿Qué pasó acá? Básicamente estamos sobre-entrenando con los mismos datos (los de entrenamiento), pero eso no mejora el desempeño con los datos de prueba. Peor aún, en la medida que más y más sobreajusta, peor es la capacidad de predecir nuevos datos, así que la exactitud de testeo se va degradando. \n",
    "<br> <img src=https://generationiron.com/wp-content/uploads/2017/12/Let-These-Guys-Be-The-Reminder-To-Never-Skip-Leg-Days.jpg\n",
    "          width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04350aa4",
   "metadata": {},
   "source": [
    "<b>CONCLUSIÓN</b>\n",
    "\n",
    "Este apunte ha profundizado en distintos modelos de clasificación, pasando por árboles de decisión, vecinos cercanos, máquina de vector soporte y redes neuronales artificiales, las que sumadas a regresión logística conforman un set de herramientas que disponemos cuando enfrentamos problemas de clasificación. Podríamos haber explorado muchos otros modelos. \n",
    "\n",
    "Nos queda mucho por profundizar respecto a algunos de los modelos que hemos visto. Trataremos de desarrollar un poco más en la próxima clase, especialmente sobre Redes Neuronales Artificiales, que aún no hemos optimizado prácticamente nada. Sin embargo, se escapa del alcance de la asignatura analizar en detalle cada uno de estos modelos, lo importante es que sepamos como usarlos más que cómo funcionan internamente. Especialmente que sepamos organizar nuestros experimentos, de forma de que, sin importar el modelo, estemos a resguardo de alcanzar conclusiones erradas producto del sobreajuste. \n",
    "\n",
    "Este apunte aborda temas teóricos que deben ser estudiados en la asignatura, a la vez que provee de material práctico en la forma de código en Python y uso de librerías y recursos. No reemplaza la búsqueda y curiosidad de quién está aprendiendo, pero es una guía sobre la cual se basará futuras evaluaciones. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
