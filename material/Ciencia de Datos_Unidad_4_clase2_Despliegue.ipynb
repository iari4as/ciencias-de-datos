{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26092db2",
   "metadata": {},
   "source": [
    "<head><b><center>CIENCIA DE DATOS</center>\n",
    "<center>Entregables para el Despliegue</center></b>\n",
    "    \n",
    "\n",
    "<center>Profesor: Gabriel Jara </center></head><br>\n",
    "El presente Jupyter Notebook busca abordar el despliegue de productos de la ciencia de datos, en la forma de: pipeline de pre-procesamiento y modelo predictivo listo para la explotación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0da7127",
   "metadata": {},
   "source": [
    "<b>Pipeline de pre-procesamiento: </b>\n",
    "\n",
    "Organizamos nuestro pre-procesamiento como una serie de transformaciones, las que luego utilizamos en secuencia usando un <b>Pipeline</b> de <i>scikit-learn</i>. Este enfoque nos permite encadenar pasos como la eliminación de columnas, codificación de variables categóricas y limpieza de datos de manera estructurada y reutilizable. Al definir el pipeline, aseguramos que las mismas transformaciones se apliquen de forma consistente tanto a los datos de entrenamiento como a los de prueba o producción.\n",
    "\n",
    "Para guardar el pipeline, así como también algunos modelos, usaremos <b>joblib</b>, biblioteca de Python diseñada para guardar y cargar objetos complejos de manera eficiente, especialmente utilizada en proyectos de ciencia de datos y machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a7ef78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline_titanic.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "\n",
    "# Creamos un transformador para eliminar columnas específicas\n",
    "class EliminarColumnas(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columnas):\n",
    "        self.columnas = columnas\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Eliminamos las columnas indicadas\n",
    "        return X.drop(columns=self.columnas)\n",
    "\n",
    "# Creamos un transformador para mapear el sexo a números\n",
    "class CodificarSexo(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convertimos 'Sex' a 0 y 1\n",
    "        X = X.copy()\n",
    "        X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n",
    "        return X\n",
    "\n",
    "# Creamos un transformador para eliminar filas con valores faltantes\n",
    "class EliminarNulos(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Eliminamos filas con nulos\n",
    "        return X.dropna()\n",
    "\n",
    "# Definimos el pipeline de procesamiento\n",
    "pipeline_titanic = Pipeline([\n",
    "    ('eliminar_columnas', EliminarColumnas(['Name', 'Ticket', 'Cabin', 'Embarked'])),\n",
    "    ('codificar_sexo', CodificarSexo()),\n",
    "    ('eliminar_nulos', EliminarNulos())\n",
    "])\n",
    "\n",
    "# Guardamos el pipeline para uso futuro\n",
    "joblib.dump(pipeline_titanic, 'pipeline_titanic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf0725",
   "metadata": {},
   "source": [
    "Hemos guardado el Pipeline, luego podemos cargarlo a otro entorno y así utilizarlo para obtener la misma transformación sobre los datos. Eso sí, es necesario también volver a definir las transformaciones, por lo cual lo más simple es importarlas desde un módulo propio. \n",
    "\n",
    "Así, podemos ejecutar la siguiente celda de forma independiente de la anterior. Puede probar reiniciando el kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717e44b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformadores_titanic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Necesitamos recuperar los transformadores que usa el pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformadores_titanic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EliminarColumnas, CodificarSexo, EliminarNulos\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Recuperamos también el pipeline \u001b[39;00m\n\u001b[32m      9\u001b[39m pipeline_titanic = joblib.load(\u001b[33m'\u001b[39m\u001b[33mpipeline_titanic.pkl\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformadores_titanic'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Necesitamos recuperar los transformadores que usa el pipeline\n",
    "from transformadores_titanic import EliminarColumnas, CodificarSexo, EliminarNulos\n",
    "\n",
    "# Recuperamos también el pipeline \n",
    "pipeline_titanic = joblib.load('pipeline_titanic.pkl')\n",
    "\n",
    "# Función para cargar los datos\n",
    "def cargar_datos(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# Aplicamos el pipeline al DataFrame con los datos del Titanic\n",
    "df = cargar_datos(\"Titanic.csv\")\n",
    "df_procesado = pipeline_titanic.fit_transform(df)\n",
    "\n",
    "# Convertimos a arreglo NumPy\n",
    "datos = df_procesado.to_numpy()\n",
    "\n",
    "# Separamos X e y\n",
    "y = datos[:,1]\n",
    "X = datos[:,2:]\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578783a7",
   "metadata": {},
   "source": [
    "<b>Guardando Modelos sklearn: </b>\n",
    "\n",
    "Partamos buscando un buen clasificador de tipo árbol, y guardemoslo usando joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ddc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "import joblib\n",
    "\n",
    "# En esta celda sólo vamos a entrenar, no usaremos testeo\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Definimos el modelo de clasificación\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Definimos la grilla de parámetros que vamos a probar\n",
    "param_grid = {'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,None],\n",
    "              'criterion': ['gini','entropy'],\n",
    "              'min_samples_split': [2,3,4,5,6,7,8],\n",
    "              'min_samples_leaf': [1,2,3,4,5,6],\n",
    "              'ccp_alpha': [0,0.0001,0.001,0.01,0.1,1]\n",
    "             }\n",
    "\n",
    "# Realizamos búsqueda de grilla con validación cruzada de 5 carpetas\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "# Ajustar el modelo con búsqueda de grilla\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimimos mejores parámetros y mejor puntaje\n",
    "print(\"Mejores parámetros: \", grid_search.best_params_)\n",
    "print(\"Mejor puntaje: \", grid_search.best_score_)\n",
    "\n",
    "# Volvemos a entrenar el modelo, con mejores parámetros\n",
    "clf = DecisionTreeClassifier(max_depth= grid_search.best_params_['max_depth'],\n",
    "                             criterion= grid_search.best_params_['criterion'],\n",
    "                             min_samples_split= grid_search.best_params_['min_samples_split'],\n",
    "                             min_samples_leaf= grid_search.best_params_['min_samples_leaf'],\n",
    "                             ccp_alpha=grid_search.best_params_['ccp_alpha'])\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Guardamos el modelo entrenado, listo para usar más adelante.\n",
    "joblib.dump(model, \"modelo_titanic_tree.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca621f09",
   "metadata": {},
   "source": [
    "Hemos guardado el modelo como archivo <b>pkl</b> (abreviación de pickle), forma de serializar objetos de Python, es decir, convertirlos a un formato binario que puede guardarse en un archivo y luego reconstruirse exactamente igual. \n",
    "\n",
    "La siguiente celda usa los recursos previamente guardados, tanto pipeline como modelo de clasificación. Por lo mismo, se puede interrumpir el kernel del jupytr y retomar desde la siguiente celda, sin problema. El modelo se cargará ya entrenado, listo para usar en nuevas predicciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4530fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import joblib\n",
    "from transformadores_titanic import EliminarColumnas, CodificarSexo, EliminarNulos\n",
    "\n",
    "# Cargamos nuevos datos y los preprocesamos con el pipeline\n",
    "pipeline = joblib.load(\"pipeline_titanic.pkl\")\n",
    "df_nuevo = pd.read_csv(\"Titanic.csv\")\n",
    "df_procesado = pipeline.transform(df_nuevo)\n",
    "\n",
    "# Convertimos a arreglo NumPy\n",
    "datos = df_procesado.to_numpy()\n",
    "\n",
    "# Separamos X e y\n",
    "y = datos[:,1]\n",
    "X = datos[:,2:]\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "\n",
    "# Esta vez sólo usaremos los datos de testeo, no necesitamos entrenamiento.\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Cargamos el modelo\n",
    "modelo = joblib.load(\"modelo_titanic_tree.pkl\")\n",
    "\n",
    "# ¿Cómo sabemos qué modelo tenemos guardado?\n",
    "# Podemos consultarlo directo al propio modelo\n",
    "print('TIPO DE MODELO:',type(modelo))\n",
    "print('HIPERPARÁMETROS',modelo.get_params())\n",
    "\n",
    "# Reportamos resultados predictivos sobre la muestra de testeo\n",
    "y_hat = modelo.predict(X_test)\n",
    "print('La exactitud de testeo del modelo guardado es:',modelo.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31492b",
   "metadata": {},
   "source": [
    "<b>Guardemos una ANN de Keras:</b>\n",
    "\n",
    "Keras no está diseñado para trabajar con joblib, pero por otro lado no es necesario dado que tiene sus propios métodos para almacenar y cargar modelos. En la siguiente celda creamos, entrenamos y guardamos una red neuronal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded581c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from transformadores_titanic import EliminarColumnas, CodificarSexo, EliminarNulos\n",
    "\n",
    "# Cargamos nuevos datos y los preprocesamos con el pipeline\n",
    "pipeline = joblib.load(\"pipeline_titanic.pkl\")\n",
    "df_nuevo = pd.read_csv(\"Titanic.csv\")\n",
    "df_procesado = pipeline.transform(df_nuevo)\n",
    "\n",
    "# Convertimos a arreglo NumPy\n",
    "datos = df_procesado.to_numpy()\n",
    "\n",
    "# Separamos X e y\n",
    "y = datos[:,1]\n",
    "X = datos[:,2:]\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "\n",
    "# Esta vez sólo usaremos los datos de entrenamiento, no necesitamos testeo.\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Modelo de Red Neuronal, nada especial\n",
    "model = Sequential()\n",
    "entrada_dim = len(X_train[0])\n",
    "model.add(Dense(units=32, activation='relu', input_dim=entrada_dim))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=1, verbose = 0)\n",
    "\n",
    "# Guardamos el modelo ya entrenado, para usarlo más adelante\n",
    "model.save(\"modelo_titanic.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3b9a4",
   "metadata": {},
   "source": [
    "La siguiente celda se puede ejecutar independiente de lo anterior, tanto el preprocesamiento como el modelo se recuperan como objetos previamente guardados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa11ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformadores_titanic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformadores_titanic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EliminarColumnas, CodificarSexo, EliminarNulos\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Cargamos nuevos datos y los preprocesamos con el pipeline\u001b[39;00m\n\u001b[32m      9\u001b[39m pipeline = joblib.load(\u001b[33m\"\u001b[39m\u001b[33mpipeline_titanic.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformadores_titanic'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "from transformadores_titanic import EliminarColumnas, CodificarSexo, EliminarNulos\n",
    "\n",
    "# Cargamos nuevos datos y los preprocesamos con el pipeline\n",
    "pipeline = joblib.load(\"pipeline_titanic.pkl\")\n",
    "df_nuevo = pd.read_csv(\"Titanic.csv\")\n",
    "df_procesado = pipeline.transform(df_nuevo)\n",
    "\n",
    "# Convertimos a arreglo NumPy\n",
    "datos = df_procesado.to_numpy()\n",
    "\n",
    "# Separamos X e y\n",
    "y = datos[:,1]\n",
    "X = datos[:,2:]\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "\n",
    "# Esta vez sólo usaremos los datos de testeo, no necesitamos entrenamiento.\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Recuperamos el modelo\n",
    "modelo = load_model(\"modelo_titanic.keras\")\n",
    "\n",
    "# Averiguemos qué modelo es este\n",
    "modelo.summary()\n",
    "\n",
    "# Evaluamos el modelo con datos de testeo\n",
    "loss, accuracy = modelo.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69c0b1",
   "metadata": {},
   "source": [
    "<b>Conclusión:</b>\n",
    "\n",
    "Hemos encapsulado el preprocesamiento y los modelos en productos entregables, que podemos cargar a nuevos ambientes para poder seguir utilizándolo. Si bien nuestro pipeline sólo consideró el pre-procesamiento, podríamos incluir desde la captura de datos hasta la generación y entrenamiento del modelo predictivo, todo dentro del mismo pipeline. \n",
    "\n",
    "Esto resulta fundamental para la eventual integración de nuestros modelos predictivos en sistemas de información que los pongan disponibles para la operación, facilitando una integración confiable, limpia y escalable de los modelos en producción."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
